{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca2213c2-6abc-4340-853a-7ab1e06e68d3",
   "metadata": {},
   "source": [
    "# Getting started\n",
    "\n",
    "In this notebook, we use `TorchSurv` to train a model that predicts relative risk of breast cancer recurrence. We use a public data set, the [German Breast Cancer Study Group 2 (GBSG2)](https://paperswithcode.com/dataset/gbsg2). After training the model, we evaluate the predictive performance using evaluation metrics implemented in `TorchSurv`.\n",
    "\n",
    "\n",
    "We first load the dataset using the package [lifelines](https://lifelines.readthedocs.io/en/latest/). The GBSG2 dataset contains features and recurrence free survival time (in days) for 686 women undergoing hormonal treatment. \n",
    "\n",
    "### Dependencies\n",
    "\n",
    "To run this notebook, dependencies must be installed. the recommended method is to use our developpment conda environment (**preffered**). Instruction can be found [here](https://opensource.nibr.com/torchsurv/devnotes.html#set-up-a-development-environment-via-conda) to install all optional dependancies. The other method is to install only required packages using the command line below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "160c8e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install only required packages (optional)\n",
    "# %pip install lifelines\n",
    "# %pip install matplotlib\n",
    "# %pip install sklearn\n",
    "# %pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "013dbcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2601dd00-7bd2-49d5-9bdf-a84205872890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lifelines\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Our package\n",
    "from torchsurv.loss.cox import neg_partial_log_likelihood\n",
    "from torchsurv.loss.weibull import neg_log_likelihood, log_hazard, survival_function\n",
    "from torchsurv.metrics.brier_score import BrierScore\n",
    "from torchsurv.metrics.cindex import ConcordanceIndex\n",
    "from torchsurv.metrics.auc import Auc\n",
    "from torchsurv.stats.kaplan_meier import KaplanMeierEstimator\n",
    "\n",
    "# PyTorch boilerplate - see https://github.com/Novartis/torchsurv/blob/main/docs/notebooks/helpers_introduction.py\n",
    "from helpers_introduction import Custom_dataset, plot_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57a833cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Issue with eager mode\n",
    "# torch._dynamo.config.suppress_errors = True  # Suppress inductor errors\n",
    "# torch._dynamo.reset()  # Reset the backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7a98ea2-100f-43ef-8c45-c786ddcd313e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA-enabled GPU/TPU is available.\n"
     ]
    }
   ],
   "source": [
    "# Constant parameters accross models\n",
    "# Detect available accelerator; Downgrade batch size if only CPU available\n",
    "if any([torch.cuda.is_available(), torch.backends.mps.is_available()]):\n",
    "    print(\"CUDA-enabled GPU/TPU is available.\")\n",
    "    BATCH_SIZE = 128  # batch size for training\n",
    "else:\n",
    "    print(\"No CUDA-enabled GPU found, using CPU.\")\n",
    "    BATCH_SIZE = 32  # batch size for training\n",
    "\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 1e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dd4c6e-2934-44f5-88fa-1d9d02032fc3",
   "metadata": {},
   "source": [
    "## Dataset overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1df49737-dc02-4d6b-acd7-d03b79f18a29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>horTh</th>\n",
       "      <th>age</th>\n",
       "      <th>menostat</th>\n",
       "      <th>tsize</th>\n",
       "      <th>tgrade</th>\n",
       "      <th>pnodes</th>\n",
       "      <th>progrec</th>\n",
       "      <th>estrec</th>\n",
       "      <th>time</th>\n",
       "      <th>cens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no</td>\n",
       "      <td>70</td>\n",
       "      <td>Post</td>\n",
       "      <td>21</td>\n",
       "      <td>II</td>\n",
       "      <td>3</td>\n",
       "      <td>48</td>\n",
       "      <td>66</td>\n",
       "      <td>1814</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yes</td>\n",
       "      <td>56</td>\n",
       "      <td>Post</td>\n",
       "      <td>12</td>\n",
       "      <td>II</td>\n",
       "      <td>7</td>\n",
       "      <td>61</td>\n",
       "      <td>77</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yes</td>\n",
       "      <td>58</td>\n",
       "      <td>Post</td>\n",
       "      <td>35</td>\n",
       "      <td>II</td>\n",
       "      <td>9</td>\n",
       "      <td>52</td>\n",
       "      <td>271</td>\n",
       "      <td>712</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yes</td>\n",
       "      <td>59</td>\n",
       "      <td>Post</td>\n",
       "      <td>17</td>\n",
       "      <td>II</td>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>29</td>\n",
       "      <td>1807</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no</td>\n",
       "      <td>73</td>\n",
       "      <td>Post</td>\n",
       "      <td>35</td>\n",
       "      <td>II</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>65</td>\n",
       "      <td>772</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  horTh  age menostat  tsize tgrade  pnodes  progrec  estrec  time  cens\n",
       "0    no   70     Post     21     II       3       48      66  1814     1\n",
       "1   yes   56     Post     12     II       7       61      77  2018     1\n",
       "2   yes   58     Post     35     II       9       52     271   712     1\n",
       "3   yes   59     Post     17     II       4       60      29  1807     1\n",
       "4    no   73     Post     35     II       1       26      65   772     1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load GBSG2 dataset\n",
    "df = lifelines.datasets.load_gbsg2()\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f23ce41-c0eb-4c30-83f3-2a2d45dcf097",
   "metadata": {},
   "source": [
    "The dataset contains the categorical features: \n",
    "\n",
    "- `horTh`: hormonal therapy, a factor at two levels (yes and no).\n",
    "- `age`:  age of the patients in years.\n",
    "- `menostat`: menopausal status, a factor at two levels pre (premenopausal) and post (postmenopausal).\n",
    "- `tsize`: tumor size (in mm).\n",
    "- `tgrade`: tumor grade, a ordered factor at levels I < II < III.\n",
    "- `pnodes`: number of positive nodes.\n",
    "- `progrec`: progesterone receptor (in fmol).\n",
    "- `estrec`: estrogen receptor (in fmol).\n",
    "\n",
    "Additionally, it contains our survival targets:\n",
    "\n",
    "- `time`: recurrence free survival time (in days).\n",
    "- `cens`: censoring indicator (0- censored, 1- event).\n",
    "\n",
    "One common approach is to use a [one hot encoder](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) to convert them into numerical features. We then seperate the dataframes into features `X` and labels `y`. The following code also partitions the labels and features into training and testing cohorts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34132fea-daa6-46a5-8429-16df73886a51",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a5fd9ef-2643-46b7-9c98-05ff919026ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>tsize</th>\n",
       "      <th>pnodes</th>\n",
       "      <th>progrec</th>\n",
       "      <th>estrec</th>\n",
       "      <th>time</th>\n",
       "      <th>cens</th>\n",
       "      <th>horTh_yes</th>\n",
       "      <th>menostat_Pre</th>\n",
       "      <th>tgrade_II</th>\n",
       "      <th>tgrade_III</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>1814.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>271.0</td>\n",
       "      <td>712.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1807.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>73.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>772.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  tsize  pnodes  progrec  estrec    time  cens  horTh_yes  \\\n",
       "0  70.0   21.0     3.0     48.0    66.0  1814.0   1.0        0.0   \n",
       "1  56.0   12.0     7.0     61.0    77.0  2018.0   1.0        1.0   \n",
       "2  58.0   35.0     9.0     52.0   271.0   712.0   1.0        1.0   \n",
       "3  59.0   17.0     4.0     60.0    29.0  1807.0   1.0        1.0   \n",
       "4  73.0   35.0     1.0     26.0    65.0   772.0   1.0        0.0   \n",
       "\n",
       "   menostat_Pre  tgrade_II  tgrade_III  \n",
       "0           0.0        1.0         0.0  \n",
       "1           0.0        1.0         0.0  \n",
       "2           0.0        1.0         0.0  \n",
       "3           0.0        1.0         0.0  \n",
       "4           0.0        1.0         0.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_onehot = pd.get_dummies(df, columns=[\"horTh\", \"menostat\", \"tgrade\"]).astype(\"float\")\n",
    "df_onehot.drop(\n",
    "    [\"horTh_no\", \"menostat_Post\", \"tgrade_I\"],\n",
    "    axis=1,\n",
    "    inplace=True,\n",
    ")\n",
    "df_onehot.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f8b7f3b-fb2a-4d74-ac99-8f6390b2f5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Sample size) Training:336 | Validation:144 |Testing:206\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = train_test_split(df_onehot, test_size=0.3)\n",
    "df_train, df_val = train_test_split(df_train, test_size=0.3)\n",
    "print(\n",
    "    f\"(Sample size) Training:{len(df_train)} | Validation:{len(df_val)} |Testing:{len(df_test)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ad6603-0dff-4991-992a-081ba9a4fafa",
   "metadata": {},
   "source": [
    "Let us setup the dataloaders for training, validation and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "326c03fc-91f1-493b-a9ba-820de17fb2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "dataloader_train = DataLoader(\n",
    "    Custom_dataset(df_train), batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "dataloader_val = DataLoader(\n",
    "    Custom_dataset(df_val), batch_size=len(df_val), shuffle=False\n",
    ")\n",
    "dataloader_test = DataLoader(\n",
    "    Custom_dataset(df_test), batch_size=len(df_test), shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "570386fb-f0ea-4061-bae2-11b274e7f851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (shape)    = torch.Size([128, 9])\n",
      "num_features = 9\n",
      "event        = torch.Size([128])\n",
      "time         = torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "x, (event, time) = next(iter(dataloader_train))\n",
    "num_features = x.size(1)\n",
    "\n",
    "print(f\"x (shape)    = {x.shape}\")\n",
    "print(f\"num_features = {num_features}\")\n",
    "print(f\"event        = {event.shape}\")\n",
    "print(f\"time         = {time.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b53d40d-d2c4-4dd7-bb85-97d4e946c356",
   "metadata": {},
   "source": [
    "## Section 1: Cox proportional hazards model\n",
    "\n",
    "In this section, we use the [Cox proportional hazards model](../_autosummary/torchsurv.loss.cox.html). Given covariate $x_{i}$, the hazard of patient $i$ has the form\n",
    "$$\n",
    "\\lambda (t|x_{i}) =\\lambda_{0}(t)\\theta(x_{i})\n",
    "$$\n",
    "The baseline hazard $\\lambda_{0}(t)$ is identical across subjects (i.e., has no dependency on $i$). The subject-specific risk of event occurrence is captured through the relative hazards $\\{\\theta(x_{i})\\}_{i = 1, \\dots, N}$.\n",
    "\n",
    "We train a multi-layer perceptron (MLP) to model the subject-specific risk of event occurrence, i.e., the log relative hazards $\\log\\theta(x_{i})$. Patients with lower recurrence time are assumed to have higher risk of event. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46343fe0",
   "metadata": {},
   "source": [
    "### Section 1.1: MLP model for log relative hazards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c2bd89a-c90a-4795-aab5-b5c21906a0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate Weibull model\n",
    "cox_model = torch.nn.Sequential(\n",
    "    torch.nn.BatchNorm1d(num_features),  # Batch normalization\n",
    "    torch.nn.Linear(num_features, 32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(),\n",
    "    torch.nn.Linear(32, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(),\n",
    "    torch.nn.Linear(64, 1),  # Estimating log hazards for Cox models\n",
    ")\n",
    "\n",
    "# Compile model for faster performance\n",
    "cox_model = torch.compile(cox_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c90244",
   "metadata": {},
   "source": [
    "### Section 1.2: MLP model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7889dc1-1cfa-424e-a586-481cbc789581",
   "metadata": {},
   "outputs": [
    {
     "ename": "InductorError",
     "evalue": "CppCompileError: C++ compile error\n\nCommand:\nclang++ /var/folders/kh/nrw1152x2dxb4rn5yvbvlg5h0000gn/T/torchinductor_corolth1/qc/cqch7lxqeus2nlkbz45cuqms723a6hy7nd5iyglyd5apmnd66qyo.cpp -D TORCH_INDUCTOR_CPP_WRAPPER -D STANDALONE_TORCH_HEADER -D C10_USING_CUSTOM_GENERATED_MACROS -D CPU_CAPABILITY_NEON -D AT_BUILD_ARM_VEC256_WITH_SLEEF -shared -fPIC -undefined dynamic_lookup -O3 -DNDEBUG -fno-trapping-math -funsafe-math-optimizations -ffinite-math-only -fno-signed-zeros -fno-math-errno -fno-finite-math-only -fno-unsafe-math-optimizations -ffp-contract=off -Wall -std=c++17 -Wno-unused-variable -Wno-unknown-pragmas -Werror=ignored-optimization-argument -Xclang -fopenmp -I/Users/corolth1/anaconda3/envs/torchsurv/include/python3.11 -I/Users/corolth1/anaconda3/envs/torchsurv/lib/python3.11/site-packages/torch/include -I/Users/corolth1/anaconda3/envs/torchsurv/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/Users/corolth1/anaconda3/envs/torchsurv/include -D_GLIBCXX_USE_CXX11_ABI=0 -lomp -L/Users/corolth1/anaconda3/envs/torchsurv/lib -L/Users/corolth1/anaconda3/envs/torchsurv/lib/python3.11/site-packages/torch/lib -o /var/folders/kh/nrw1152x2dxb4rn5yvbvlg5h0000gn/T/torchinductor_corolth1/qc/cqch7lxqeus2nlkbz45cuqms723a6hy7nd5iyglyd5apmnd66qyo.so\n\nOutput:\nIn file included from /var/folders/kh/nrw1152x2dxb4rn5yvbvlg5h0000gn/T/torchinductor_corolth1/qc/cqch7lxqeus2nlkbz45cuqms723a6hy7nd5iyglyd5apmnd66qyo.cpp:2:\n/var/folders/kh/nrw1152x2dxb4rn5yvbvlg5h0000gn/T/torchinductor_corolth1/pi/cpicxudqmdsjh5cm4klbtbrvy2cxwr7whxl3md2zzdjdf3orvfdf.h:3:10: fatal error: 'algorithm' file not found\n    3 | #include <algorithm>\n      |          ^~~~~~~~~~~\n1 error generated.\n\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInductorError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m x, (event, time) \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 16\u001b[0m log_hz \u001b[38;5;241m=\u001b[39m \u001b[43mcox_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shape = (16, 1)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m neg_partial_log_likelihood(log_hz, event, time, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/torchsurv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torchsurv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/torchsurv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:663\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ShortenTraceback \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    661\u001b[0m     \u001b[38;5;66;03m# Failures in the backend likely don't have useful\u001b[39;00m\n\u001b[1;32m    662\u001b[0m     \u001b[38;5;66;03m# data in the TorchDynamo frames, so we strip them out.\u001b[39;00m\n\u001b[0;32m--> 663\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mremove_dynamo_frames() \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# see TORCHDYNAMO_VERBOSE=1\u001b[39;00m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    665\u001b[0m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[1;32m    666\u001b[0m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchsurv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:760\u001b[0m, in \u001b[0;36m_compile_fx_inner\u001b[0;34m(gm, example_inputs, **graph_kwargs)\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 760\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InductorError(e, currentframe())\u001b[38;5;241m.\u001b[39mwith_traceback(\n\u001b[1;32m    761\u001b[0m         e\u001b[38;5;241m.\u001b[39m__traceback__\n\u001b[1;32m    762\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    764\u001b[0m     TritonBundler\u001b[38;5;241m.\u001b[39mend_compile()\n",
      "File \u001b[0;32m~/anaconda3/envs/torchsurv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:745\u001b[0m, in \u001b[0;36m_compile_fx_inner\u001b[0;34m(gm, example_inputs, **graph_kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m TritonBundler\u001b[38;5;241m.\u001b[39mbegin_compile()\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 745\u001b[0m     mb_compiled_graph \u001b[38;5;241m=\u001b[39m \u001b[43mfx_codegen_and_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_to_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgraph_kwargs\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m mb_compiled_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    749\u001b[0m     mb_compiled_graph\u001b[38;5;241m.\u001b[39m_time_taken_ns \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime_ns() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/anaconda3/envs/torchsurv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:1295\u001b[0m, in \u001b[0;36mfx_codegen_and_compile\u001b[0;34m(gm, example_inputs, inputs_to_check, **graph_kwargs)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompile_fx_subproc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _SubprocessFxCompile\n\u001b[1;32m   1293\u001b[0m     scheme \u001b[38;5;241m=\u001b[39m _SubprocessFxCompile()\n\u001b[0;32m-> 1295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscheme\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodegen_and_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_to_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torchsurv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:1197\u001b[0m, in \u001b[0;36m_InProcessFxCompile.codegen_and_compile\u001b[0;34m(self, gm, example_inputs, inputs_to_check, graph_kwargs)\u001b[0m\n\u001b[1;32m   1184\u001b[0m             compiled_fn \u001b[38;5;241m=\u001b[39m AotCodeCompiler\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m   1185\u001b[0m                 graph,\n\u001b[1;32m   1186\u001b[0m                 wrapper_code\u001b[38;5;241m.\u001b[39mvalue,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1194\u001b[0m                 ],\n\u001b[1;32m   1195\u001b[0m             )\n\u001b[1;32m   1196\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1197\u001b[0m         compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_to_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcall\n\u001b[1;32m   1199\u001b[0m num_bytes, nodes_num_elem, node_runtimes \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mcount_bytes()\n\u001b[1;32m   1200\u001b[0m metrics\u001b[38;5;241m.\u001b[39mnum_bytes_accessed \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m num_bytes\n",
      "File \u001b[0;32m~/anaconda3/envs/torchsurv/lib/python3.11/site-packages/torch/_inductor/graph.py:2083\u001b[0m, in \u001b[0;36mGraphLowering.compile_to_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2076\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompile_to_module\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ModuleType:\n\u001b[1;32m   2077\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\n\u001b[1;32m   2078\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGraphLowering.compile_to_module\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2079\u001b[0m         phase_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_gen\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2080\u001b[0m         log_pt2_compile_event\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   2081\u001b[0m         dynamo_compile_column_us\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minductor_code_gen_cumulative_compile_time_us\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2082\u001b[0m     ):\n\u001b[0;32m-> 2083\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compile_to_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torchsurv/lib/python3.11/site-packages/torch/_inductor/graph.py:2130\u001b[0m, in \u001b[0;36mGraphLowering._compile_to_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2124\u001b[0m     trace_structured(\n\u001b[1;32m   2125\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minductor_output_code\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2126\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m\"\u001b[39m: path},\n\u001b[1;32m   2127\u001b[0m         payload_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m: wrapper_code\u001b[38;5;241m.\u001b[39mvalue,\n\u001b[1;32m   2128\u001b[0m     )\n\u001b[1;32m   2129\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPyCodeCache.load_by_key_path\u001b[39m\u001b[38;5;124m\"\u001b[39m, log_pt2_compile_event\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m-> 2130\u001b[0m     mod \u001b[38;5;241m=\u001b[39m \u001b[43mPyCodeCache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_by_key_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlinemap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlinemap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   2134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstants\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorchbind_constants\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2135\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_key \u001b[38;5;241m=\u001b[39m key\n\u001b[1;32m   2137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_path \u001b[38;5;241m=\u001b[39m path\n",
      "File \u001b[0;32m~/anaconda3/envs/torchsurv/lib/python3.11/site-packages/torch/_inductor/codecache.py:2747\u001b[0m, in \u001b[0;36mPyCodeCache.load_by_key_path\u001b[0;34m(cls, key, path, linemap, attrs)\u001b[0m\n\u001b[1;32m   2744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m linemap \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2745\u001b[0m     linemap \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 2747\u001b[0m mod \u001b[38;5;241m=\u001b[39m \u001b[43m_reload_python_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2749\u001b[0m \u001b[38;5;66;03m# unzip into separate lines/nodes lists\u001b[39;00m\n\u001b[1;32m   2750\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mlinemaps[path] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mlinemap))\n",
      "File \u001b[0;32m~/anaconda3/envs/torchsurv/lib/python3.11/site-packages/torch/_inductor/runtime/compile_tasks.py:36\u001b[0m, in \u001b[0;36m_reload_python_module\u001b[0;34m(key, path)\u001b[0m\n\u001b[1;32m     34\u001b[0m mod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__file__\u001b[39m \u001b[38;5;241m=\u001b[39m path\n\u001b[1;32m     35\u001b[0m mod\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;241m=\u001b[39m key  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__dict__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__dict__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m sys\u001b[38;5;241m.\u001b[39mmodules[mod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m] \u001b[38;5;241m=\u001b[39m mod\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mod\n",
      "File \u001b[0;32m/var/folders/kh/nrw1152x2dxb4rn5yvbvlg5h0000gn/T/torchinductor_corolth1/r3/cr34l7ojypcwqyxlw3p42muhgi44nugdfhg2sazj4mc65im3fhp3.py:222\u001b[0m\n\u001b[1;32m    165\u001b[0m cpp_fused_native_dropout_relu_threshold_backward_2 \u001b[38;5;241m=\u001b[39m async_compile\u001b[38;5;241m.\u001b[39mcpp_pybinding([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconst int64_t*\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconst float*\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbool*\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat*\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbool*\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124m#include \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/var/folders/kh/nrw1152x2dxb4rn5yvbvlg5h0000gn/T/torchinductor_corolth1/pi/cpicxudqmdsjh5cm4klbtbrvy2cxwr7whxl3md2zzdjdf3orvfdf.h\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124mextern \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  void kernel(const int64_t* in_ptr0,\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124m}\u001b[39m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124m'''\u001b[39m)\n\u001b[1;32m    203\u001b[0m cpp_fused_add_3 \u001b[38;5;241m=\u001b[39m async_compile\u001b[38;5;241m.\u001b[39mcpp_pybinding([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconst int64_t*\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint64_t*\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124m#include \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/var/folders/kh/nrw1152x2dxb4rn5yvbvlg5h0000gn/T/torchinductor_corolth1/pi/cpicxudqmdsjh5cm4klbtbrvy2cxwr7whxl3md2zzdjdf3orvfdf.h\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124mextern \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  void kernel(const int64_t* in_ptr0,\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124m}\u001b[39m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;124m'''\u001b[39m)\n\u001b[0;32m--> 222\u001b[0m \u001b[43masync_compile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m async_compile\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall\u001b[39m(args):\n",
      "File \u001b[0;32m~/anaconda3/envs/torchsurv/lib/python3.11/site-packages/torch/_inductor/async_compile.py:424\u001b[0m, in \u001b[0;36mAsyncCompile.wait\u001b[0;34m(self, scope)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_compile_threads() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\n\u001b[1;32m    419\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masync_compile.wait\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    420\u001b[0m         log_pt2_compile_event\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    421\u001b[0m         dynamo_compile_column_us\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtriton_compile_time_us\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    422\u001b[0m         log_waitcounter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    423\u001b[0m     ):\n\u001b[0;32m--> 424\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_futures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    426\u001b[0m _compile_end()\n",
      "File \u001b[0;32m~/anaconda3/envs/torchsurv/lib/python3.11/site-packages/torch/_inductor/async_compile.py:445\u001b[0m, in \u001b[0;36mAsyncCompile._wait_futures\u001b[0;34m(self, scope)\u001b[0m\n\u001b[1;32m    443\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mset_postfix_str(key)\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 445\u001b[0m     scope[key] \u001b[38;5;241m=\u001b[39m \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BrokenProcessPool \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA compilation subprocess exited unexpectedly. This \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    449\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis likely due to a crash. To facilitate debugging, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    450\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can re-run with TORCHINDUCTOR_COMPILE_THREADS=1 \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto cause compilation to occur in the main process.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torchsurv/lib/python3.11/site-packages/torch/_inductor/codecache.py:3224\u001b[0m, in \u001b[0;36mLambdaFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, Any]:  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m-> 3224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torchsurv/lib/python3.11/site-packages/torch/_inductor/codecache.py:2242\u001b[0m, in \u001b[0;36mCppPythonBindingsCodeCache.load_pybinding_async.<locals>.future\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01mnonlocal\u001b[39;00m result\n\u001b[1;32m   2241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2242\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2243\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, ModuleType)\n\u001b[1;32m   2244\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mentry_function)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchsurv/lib/python3.11/site-packages/torch/_inductor/codecache.py:2050\u001b[0m, in \u001b[0;36mCppCodeCache.load_async.<locals>.load_fn\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lib \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2049\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m future \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2050\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2051\u001b[0m     result \u001b[38;5;241m=\u001b[39m worker_fn()\n\u001b[1;32m   2052\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torchsurv/lib/python3.11/concurrent/futures/_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m~/anaconda3/envs/torchsurv/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torchsurv/lib/python3.11/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchsurv/lib/python3.11/site-packages/torch/_inductor/codecache.py:2079\u001b[0m, in \u001b[0;36m_worker_compile_cpp\u001b[0;34m(lock_path, cpp_builder)\u001b[0m\n\u001b[1;32m   2077\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m FileLock(lock_path, timeout\u001b[38;5;241m=\u001b[39mLOCK_TIMEOUT):\n\u001b[1;32m   2078\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(cpp_builder\u001b[38;5;241m.\u001b[39mget_target_file_path()):\n\u001b[0;32m-> 2079\u001b[0m         \u001b[43mcpp_builder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torchsurv/lib/python3.11/site-packages/torch/_inductor/cpp_builder.py:1601\u001b[0m, in \u001b[0;36mCppBuilder.build\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1598\u001b[0m _create_if_dir_not_exist(_build_tmp_dir)\n\u001b[1;32m   1600\u001b[0m build_cmd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_command_line()\n\u001b[0;32m-> 1601\u001b[0m \u001b[43mrun_compile_cmd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuild_cmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_build_tmp_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1602\u001b[0m _remove_dir(_build_tmp_dir)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchsurv/lib/python3.11/site-packages/torch/_inductor/cpp_builder.py:355\u001b[0m, in \u001b[0;36mrun_compile_cmd\u001b[0;34m(cmd_line, cwd)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_compile_cmd\u001b[39m(cmd_line: \u001b[38;5;28mstr\u001b[39m, cwd: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompile_file\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 355\u001b[0m         \u001b[43m_run_compile_cmd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd_line\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torchsurv/lib/python3.11/site-packages/torch/_inductor/cpp_builder.py:350\u001b[0m, in \u001b[0;36m_run_compile_cmd\u001b[0;34m(cmd_line, cwd)\u001b[0m\n\u001b[1;32m    340\u001b[0m     instruction \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    341\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOpenMP support not found. Please try one of the following solutions:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(1) Set the `CXX` environment variable to a compiler other than Apple clang++/g++ \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m with `include/omp.h` under it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    348\u001b[0m     )\n\u001b[1;32m    349\u001b[0m     output \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m instruction\n\u001b[0;32m--> 350\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mCppCompileError(cmd, output) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mInductorError\u001b[0m: CppCompileError: C++ compile error\n\nCommand:\nclang++ /var/folders/kh/nrw1152x2dxb4rn5yvbvlg5h0000gn/T/torchinductor_corolth1/qc/cqch7lxqeus2nlkbz45cuqms723a6hy7nd5iyglyd5apmnd66qyo.cpp -D TORCH_INDUCTOR_CPP_WRAPPER -D STANDALONE_TORCH_HEADER -D C10_USING_CUSTOM_GENERATED_MACROS -D CPU_CAPABILITY_NEON -D AT_BUILD_ARM_VEC256_WITH_SLEEF -shared -fPIC -undefined dynamic_lookup -O3 -DNDEBUG -fno-trapping-math -funsafe-math-optimizations -ffinite-math-only -fno-signed-zeros -fno-math-errno -fno-finite-math-only -fno-unsafe-math-optimizations -ffp-contract=off -Wall -std=c++17 -Wno-unused-variable -Wno-unknown-pragmas -Werror=ignored-optimization-argument -Xclang -fopenmp -I/Users/corolth1/anaconda3/envs/torchsurv/include/python3.11 -I/Users/corolth1/anaconda3/envs/torchsurv/lib/python3.11/site-packages/torch/include -I/Users/corolth1/anaconda3/envs/torchsurv/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/Users/corolth1/anaconda3/envs/torchsurv/include -D_GLIBCXX_USE_CXX11_ABI=0 -lomp -L/Users/corolth1/anaconda3/envs/torchsurv/lib -L/Users/corolth1/anaconda3/envs/torchsurv/lib/python3.11/site-packages/torch/lib -o /var/folders/kh/nrw1152x2dxb4rn5yvbvlg5h0000gn/T/torchinductor_corolth1/qc/cqch7lxqeus2nlkbz45cuqms723a6hy7nd5iyglyd5apmnd66qyo.so\n\nOutput:\nIn file included from /var/folders/kh/nrw1152x2dxb4rn5yvbvlg5h0000gn/T/torchinductor_corolth1/qc/cqch7lxqeus2nlkbz45cuqms723a6hy7nd5iyglyd5apmnd66qyo.cpp:2:\n/var/folders/kh/nrw1152x2dxb4rn5yvbvlg5h0000gn/T/torchinductor_corolth1/pi/cpicxudqmdsjh5cm4klbtbrvy2cxwr7whxl3md2zzdjdf3orvfdf.h:3:10: fatal error: 'algorithm' file not found\n    3 | #include <algorithm>\n      |          ^~~~~~~~~~~\n1 error generated.\n\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Init optimizer for Cox\n",
    "optimizer = torch.optim.Adam(cox_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Initiate empty list to store the loss on the train and validation sets\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = torch.tensor(0.0)\n",
    "    for i, batch in enumerate(dataloader_train):\n",
    "        x, (event, time) = batch\n",
    "        optimizer.zero_grad()\n",
    "        log_hz = cox_model(x)  # shape = (16, 1)\n",
    "        loss = neg_partial_log_likelihood(log_hz, event, time, reduction=\"mean\")\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.detach()\n",
    "\n",
    "    if epoch % (EPOCHS // 10) == 0:\n",
    "        print(f\"Epoch: {epoch:03}, Training loss: {epoch_loss:0.2f}\")\n",
    "\n",
    "    # Reccord loss on train and test sets\n",
    "    epoch_loss /= i + 1\n",
    "    train_losses.append(epoch_loss)\n",
    "    with torch.no_grad():\n",
    "        x, (event, time) = next(iter(dataloader_val))\n",
    "        val_losses.append(\n",
    "            neg_partial_log_likelihood(cox_model(x), event, time, reduction=\"mean\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2bdd8c-f84c-4003-98f4-220ddab518d1",
   "metadata": {},
   "source": [
    "We can visualize the training and validation losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21afc248-303a-4156-8d9c-b97be3e0a56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_losses, val_losses, \"Cox\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd881d14-9646-48e0-bcc3-f29be358161f",
   "metadata": {},
   "source": [
    "### Section 1.3: Cox proportional hazards model evaluation\n",
    "\n",
    "We evaluate the predictive performance of the model using \n",
    "\n",
    "* the [concordance index](../_autosummary/torchsurv.metrics.cindex.html) (C-index), which measures the the probability that a model correctly predicts which of two comparable samples will experience an event first based on their estimated risk scores,\n",
    "* the [Area Under the Receiver Operating Characteristic Curve](../_autosummary/torchsurv.metrics.auc.html) (AUC), which measures the probability that a model correctly predicts which of two comparable samples will experience an event by time t based on their estimated risk scores.\n",
    "\n",
    "We cannot use the Brier score because this model is not able to estimate the survival function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2e7996",
   "metadata": {},
   "source": [
    "We start by evaluating the subject-specific relative hazards on the test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272a997d-a978-4e9b-bb0b-d90e4f03a530",
   "metadata": {},
   "outputs": [],
   "source": [
    "cox_model.eval()\n",
    "with torch.no_grad():\n",
    "    # test event and test time of length n\n",
    "    x, (event, time) = next(iter(dataloader_test))\n",
    "    log_hz = cox_model(x)  # log hazard of length n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bd0fe9",
   "metadata": {},
   "source": [
    "We obtain the concordance index, and its confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9ad489-9e53-40ac-8931-8941597760a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concordance index\n",
    "cox_cindex = ConcordanceIndex()\n",
    "print(\"Cox model performance:\")\n",
    "print(f\"Concordance-index   = {cox_cindex(log_hz, event, time)}\")\n",
    "print(f\"Confidence interval = {cox_cindex.confidence_interval()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507b410a",
   "metadata": {},
   "source": [
    "We can also test whether the observed concordance index is greater than 0.5. The statistical test is specified with H0: c-index = 0.5 and Ha: c-index > 0.5. The p-value of the statistical test is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d34ba82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H0: cindex = 0.5, Ha: cindex > 0.5\n",
    "print(\"p-value = {}\".format(cox_cindex.p_value(alternative=\"greater\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60919a9",
   "metadata": {},
   "source": [
    "For time-dependent prediction (e.g., 5-year mortality), the C-index is not a proper measure. Instead, it is recommended to use the AUC. The probability to correctly predicts which of two comparable patients will experience an event by 5-year based on their estimated risk scores is the AUC evaluated at 5-year (1825 days) obtained with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907312f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cox_auc = Auc()\n",
    "\n",
    "new_time = torch.tensor(1825.0)\n",
    "\n",
    "# auc evaluated at new time = 1825, 5 year\n",
    "print(f\"AUC 5-yr             = {cox_auc(log_hz, event, time, new_time=new_time)}\")\n",
    "print(f\"AUC 5-yr (conf int.) = {cox_auc.confidence_interval()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e7e69f",
   "metadata": {},
   "source": [
    "As before, we can test whether the observed Auc at 5-year is greater than 0.5. The statistical test is specified with H0: auc = 0.5 and Ha: auc > 0.5. The p-value of the statistical test is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702e5a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"AUC (p_value) = {cox_auc.p_value()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f517e6-b0a4-4fbc-aac5-b500b4aca169",
   "metadata": {},
   "source": [
    "## Section 2: Weibull accelerated failure time (AFT) model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769ddcf5",
   "metadata": {},
   "source": [
    "In this section, we use the [Weibull accelerated failure (AFT) model](../_autosummary/torchsurv.loss.weibull.html). Given covariate $x_{i}$, the hazard of patient $i$ at time $t$ has the form\n",
    "$$\n",
    "\\lambda (t|x_{i}) = \\frac{\\rho(x_{i}) } {\\lambda(x_{i}) } + \\left(\\frac{t}{\\lambda(x_{i})}\\right)^{\\rho(x_{i}) - 1}\n",
    "$$\n",
    "\n",
    "Given the hazard form, it can be shown that the event density follows a Weibull distribution parametrized by scale $\\lambda(x_{i})$ and shape $\\rho(x_{i})$. The subject-specific risk of event occurrence at time $t$ is captured through the hazards $\\{\\lambda (t|x_{i})\\}_{i = 1, \\dots, N}$. We train a multi-layer perceptron (MLP) to model the subject-specific log scale, $\\log \\lambda(x_{i})$, and the log shape, $\\log\\rho(x_{i})$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a580702e",
   "metadata": {},
   "source": [
    "### Section 2.1: MLP model for log scale and log shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b92c10-e5fb-491d-9e27-743bcffdced2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same architecture than Cox model, beside outputs dimension\n",
    "weibull_model = torch.nn.Sequential(\n",
    "    torch.nn.BatchNorm1d(num_features),  # Batch normalization\n",
    "    torch.nn.Linear(num_features, 32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(),\n",
    "    torch.nn.Linear(32, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(),\n",
    "    torch.nn.Linear(64, 2),  # Estimating log parameters for Weibull model\n",
    ")\n",
    "\n",
    "# Compile Weibull model\n",
    "weibull_model = torch.compile(weibull_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96c6985",
   "metadata": {},
   "source": [
    "### Section 2.2: MLP model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5c6f77-6245-42b0-ae48-33b57789b651",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Init optimizer for Weibull\n",
    "optimizer = torch.optim.Adam(weibull_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Initialize empty list to store loss on train and validation sets\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = torch.tensor(0.0)\n",
    "    for i, batch in enumerate(dataloader_train):\n",
    "        x, (event, time) = batch\n",
    "        optimizer.zero_grad()\n",
    "        log_params = weibull_model(x)  # shape = (16, 2)\n",
    "        loss = neg_log_likelihood(log_params, event, time, reduction=\"mean\")\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.detach()\n",
    "\n",
    "    if epoch % (EPOCHS // 10) == 0:\n",
    "        print(f\"Epoch: {epoch:03}, Training loss: {epoch_loss:0.2f}\")\n",
    "\n",
    "    # Reccord losses for the following figure\n",
    "    train_losses.append(epoch_loss)\n",
    "    with torch.no_grad():\n",
    "        x, (event, time) = next(iter(dataloader_val))\n",
    "        val_losses.append(\n",
    "            neg_log_likelihood(weibull_model(x), event, time, reduction=\"mean\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aba21b6",
   "metadata": {},
   "source": [
    "We can visualize the training and validation losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243a4fa9-f751-46e7-83f3-e623bfd3518e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_losses, val_losses, \"Weibull\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86139132-d337-47b7-a8ad-eac1e255f91d",
   "metadata": {},
   "source": [
    "### Section 2.3: Weibull AFT model evaluation\n",
    "\n",
    "We evaluate the predictive performance of the model using \n",
    "\n",
    "* the [C-index](../_autosummary/torchsurv.metrics.cindex.html), which measures the the probability that a model correctly predicts which of two comparable samples will experience an event first based on their estimated risk scores,\n",
    "* the [AUC](../_autosummary/torchsurv.metrics.auc.html), which measures the probability that a model correctly predicts which of two comparable samples will experience an event by time t based on their estimated risk scores, and\n",
    "* the [Brier score](../_autosummary/torchsurv.metrics.brier_score.html), which measures the model's calibration by calculating the mean square error between the estimated survival function and the empirical (i.e., in-sample) event status."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb226f5",
   "metadata": {},
   "source": [
    "We start by obtaining the subject-specific log hazard and survival probability at every time $t$ observed on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11599a1f-597b-4ebf-8a15-d3f9db1ebcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "weibull_model.eval()\n",
    "with torch.no_grad():\n",
    "    # event and time of length n\n",
    "    x, (event, time) = next(iter(dataloader_test))\n",
    "    log_params = weibull_model(x)  # shape = (n,2)\n",
    "\n",
    "# Compute the log hazards from weibull log parameters\n",
    "log_hz = log_hazard(log_params, time)  # shape = (n,n)\n",
    "\n",
    "# Compute the survival probability from weibull log parameters\n",
    "surv = survival_function(log_params, time)  # shape = (n,n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e309515",
   "metadata": {},
   "source": [
    "We can evaluate the concordance index, its confidence interval and the p-value of the statistical test testing whether the c-index is greater than 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd7e7a5-c909-41eb-a48f-a9c9832eb33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concordance index\n",
    "weibull_cindex = ConcordanceIndex()\n",
    "print(\"Weibull model performance:\")\n",
    "print(f\"Concordance-index   = {weibull_cindex(log_hz, event, time)}\")\n",
    "print(f\"Confidence interval = {weibull_cindex.confidence_interval()}\")\n",
    "\n",
    "# H0: cindex = 0.5, Ha: cindex >0.5\n",
    "print(f\"p-value             = {weibull_cindex.p_value(alternative = 'greater')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d985e48c",
   "metadata": {},
   "source": [
    "For time-dependent prediction (e.g., 5-year mortality), the C-index is not a proper measure. Instead, it is recommended to use the AUC. The probability to correctly predicts which of two comparable patients will experience an event by 5-year based on their estimated risk scores is the AUC evaluated at 5-year (1825 days) obtained with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4e6c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_time = torch.tensor(1825.0)\n",
    "\n",
    "# subject-specific log hazard at \\5-yr\n",
    "log_hz_t = log_hazard(log_params, time=new_time)  # shape = (n)\n",
    "weibull_auc = Auc()\n",
    "\n",
    "# auc evaluated at new time = 1825, 5 year\n",
    "print(f\"AUC 5-yr             = {weibull_auc(log_hz_t, event, time, new_time=new_time)}\")\n",
    "print(f\"AUC 5-yr (conf int.) = {weibull_auc.confidence_interval()}\")\n",
    "print(f\"AUC 5-yr (p value)   = {weibull_auc.p_value(alternative='greater')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b00e9f",
   "metadata": {},
   "source": [
    "Lastly, we can evaluate the time-dependent Brier score and the integrated Brier score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d99480-b643-4836-acd3-7614fa903543",
   "metadata": {},
   "outputs": [],
   "source": [
    "brier_score = BrierScore()\n",
    "\n",
    "# brier score at first 5 times\n",
    "print(f\"Brier score             = {brier_score(surv, event, time)[:5]}\")\n",
    "print(f\"Brier score (conf int.) = {brier_score.confidence_interval()[:,:5]}\")\n",
    "\n",
    "# integrated brier score\n",
    "print(f\"Integrated Brier score  = {brier_score.integral()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca1d08c",
   "metadata": {},
   "source": [
    "We can test whether the time-dependent Brier score is smaller than what would be expected if the survival model was not providing accurate predictions beyond random chance. We use a bootstrap permutation test and obtain the p-value with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9754fdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H0: bs = bs0, Ha: bs < bs0; where bs0 is the expected brier score if the survival model was not providing accurate predictions beyond random chance.\n",
    "\n",
    "# p-value for brier score at first 5 times\n",
    "print(f\"Brier score (p-val)        = {brier_score.p_value(alternative = 'less')[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f7e7f6-8f07-4f82-8653-8d0d2d1ed84f",
   "metadata": {},
   "source": [
    "## Section 3: Models comparison\n",
    "\n",
    "We can compare the predictive performance of the Cox proportional hazards model against the Weibull AFT model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed057468-ce75-4d3e-a825-71b55effcec8",
   "metadata": {},
   "source": [
    "### Section 3.1: Concordance index\n",
    "The statistical test is formulated as follows, H0: cindex cox = cindex weibull, Ha: cindex cox > cindex weibull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea66963f-2537-4390-bb65-c773275b292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Cox cindex     = {cox_cindex.cindex}\")\n",
    "print(f\"Weibull cindex = {weibull_cindex.cindex}\")\n",
    "print(\"p-value        = {}\".format(cox_cindex.compare(weibull_cindex)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f478e8df",
   "metadata": {},
   "source": [
    "### Section 3.2: AUC at 5-year\n",
    "\n",
    "The statistical test is formulated as follows, H0: 5-yr auc cox = 5-yr auc weibull, Ha: 5-yr auc cox > 5-yr auc weibull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4e1651",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Cox 5-yr AUC     = {cox_auc.auc}\")\n",
    "print(f\"Weibull 5-yr AUC = {weibull_auc.auc}\")\n",
    "print(\"p-value          = {}\".format(cox_auc.compare(weibull_auc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a00b69",
   "metadata": {},
   "source": [
    "## Section 4: Kaplan Meier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec8b7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Kaplan-Meier estimator\n",
    "km = KaplanMeierEstimator()\n",
    "\n",
    "# Use our observed testing dataset\n",
    "event = torch.tensor(df_test[\"cens\"].values).bool()\n",
    "time = torch.tensor(df_test[\"time\"].values)\n",
    "\n",
    "# Compute the estimator\n",
    "km(event, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57051c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot estimate\n",
    "km.plot_km()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0716ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the survival values at each time step\n",
    "km.print_survival_table()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchsurv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
