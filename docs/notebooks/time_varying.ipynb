{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing time-varying covariates\n",
    "\n",
    "In this notebook, we analyse a simulated dataset with time-varying covariates and survival outcomes. `TorchSurv` is used to train a model that predicts relative risk of subjects based on covariates observed over time. We will attempt to thoroughly explain the necessary elements to understand our implementation, but for a detailed read on time-varying survival models refer to Chapter 6 of [Dynamic Regression Models for Survival Data](https://link.springer.com/book/10.1007/0-387-33960-4). For a more brief explanation, please refer to these [slides](https://ms.uky.edu/~mai/sta635/Cox%20model.pdf). Below is a summary of the necessary information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial log likelihood for time-varying covariates\n",
    "\n",
    "### Context and statistical set-up\n",
    "\n",
    "Let $i$ e the index for some subject $i$ with a failute time denoted as $\\tau^*_i$ and $C$ be the censoring time. For the moment $C$ remains constant but there are extensions that allow for $C$ to vary over $i$. Let $\\tau_i = min(\\tau^*_i, C)$. We use $\\delta_i$ to denote whether $\\tau^*_i$ was observed. \n",
    "\n",
    "We will use $Z(t)$ to denote the value of of covariate $Z$ and time $t$. \n",
    "We use $Z(t)$ to denote the value of Z at time $t$ and $\\overline{Z}(t)$ to denote the set of covariates from the beggining up to time $t$: $ \\overline{Z}(t) = \\{ Z(s): 0 \\leq s \\leq t\\}$.\n",
    "Let $t_k$ for $k \\in \\{1, \\dots, K\\} denote the time points at which the covariates are observed. For the moment, we assume that all subjects have been observed on the same time grid. $R_k$ is the set of individuals who are at risk at $t_k$. \n",
    "\n",
    "The conditional hazard function of $T$ given $\\overline{Z}(t)$ is defined as\n",
    "$$ \\lambda(T|\\overline{Z}(t)) = Pr(T \\in [t, t+ dt)|T \\geq t, \\overline{Z}(t)), $$\n",
    "in other words, it is the probability that an event will occur in the next time instance if we have observed covariates up to time $t$ and that a subject has not yet experienced an event.\n",
    "\n",
    "The typical cox proportional hazards model with constant covariates $Z$ assumes a constant hazard ratio: $\\lambda(T|Z)= \\lambda_0(t) exp(\\beta Z)$, where $\\beta$ in an unknown set of regression parameters and $\\lambda_0(t)$ is an unspecified baseline hazard function. In this case $\\frac{\\lambda(T|Z)}{\\lambda_0(t)} = exp(\\beta Z) $. The cumlative hazard ia defined as $\\Lambda(t) = \\int_0^t \\lambda(s)ds$. \n",
    "\n",
    "In a time varying cox model, the hazard ratio is now dependant on time:\n",
    "$$ \\frac{\\lambda(t|Z)}{\\lambda_0(t)} = exp(\\beta Z(t)) $$ \n",
    "and the proportinal hazard model specifies:\n",
    "$$ \\lambda(t|Z) = \\lambda_0(t)exp(\\beta Z(t)) $$\n",
    "\n",
    "Let $i_j$ denote the label or identity of the individual who fails at time $\\tau_j$, including the value of their time-varying covariate\n",
    "during their time in the study $\\{ Z_{i_j}(t): t \\in [0, \\tau_j] \\}$. The partial likelihood is:\n",
    "$$ L (\\beta) = \\prod_j \\Big (\\frac{\\lambda(\\tau_j: Z_i(\\tau_j)))}{\\sum_{l \\in R_i} \\lambda(\\tau_j: Z_l(\\tau_j)))} \\Big),$$\n",
    "in terms of the model form:\n",
    "$$ L (\\beta) = \\prod_j \\Big (\\frac{\\exp(\\beta Z_i(\\tau_j))}{\\sum_{j \\in R_i} \\exp(\\beta Z_i(\\tau_j))} \\Big).$$\n",
    "\n",
    "Taking the log on both sides, we get the partial log-likelihood:\n",
    "$$ \\log L (\\beta) = \\sum_j \\Big (\\beta Z_i(\\tau_j)) - \\log [\\sum_{j \\in R_i} \\exp(\\beta Z_i(\\tau_j))]\\Big ). $$\n",
    "\n",
    "\n",
    "### Extension to neural networks\n",
    "\n",
    "Consider a more genera form, where we have the cox proportional hazards model:\n",
    "$$\\lambda(T|\\overline{Z}(t))= \\lambda_0(t) \\theta(Z(t))$$\n",
    "\n",
    "Additionally, consider some network that maps the input covariates $Z(t)$ to the log relative hazards: $\\log \\theta(Z(t))$.\n",
    "\n",
    "The partial likelihood with repsect to $\\theta(Z(\\tau_j))$ is written as:\n",
    "$$ \\log L(\\theta) = \\sum_j \\Big( \\log \\theta(Z_i(\\tau_j)) - \\log [\\sum_{j \\in R_i} \\theta (Z_i(\\tau_j))] \\Big).$$\n",
    "It onlu considers the covariate values at the time of event or censoring denoted as $\\tau_j$, all prior covariates are not considered.\n",
    "\n",
    "As the output of the network is set to be $\\log \\theta(Z(t))$, the code is written to account for this, to show this explicitly, set $\\phi(Z(t)) = \\log \\theta(Z(t))$ and write the log likelihood in terms oh $phi$:\n",
    "\n",
    "$$ \\log L(\\theta) = \\sum_j \\Big( \\phi(Z_i(\\tau_j)) - \\log [\\sum_{j \\in R_i} \\exp \\phi(Z_i(\\tau_j))] \\Big).$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "\n",
    "To run this notebook, dependencies must be installed. the recommended method is to use our developpment conda environment (**preffered**). Instruction can be found [here](https://opensource.nibr.com/torchsurv/devnotes.html#set-up-a-development-environment-via-conda) to install all optional dependancies. The other method is to install only required packages using the command line below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install only required packages (optional)\n",
    "# %pip install lifelines\n",
    "# %pip install matplotlib\n",
    "# %pip install sklearn\n",
    "# %pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Our package\n",
    "#from torchsurv.loss.time_varying import neg_partial_log_likelihood2\n",
    "\n",
    "# PyTorch boilerplate - see https://github.com/Novartis/torchsurv/blob/main/docs/notebooks/helpers_introduction.py\n",
    "from helpers_introduction import Custom_dataset, plot_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating a dataset: first approach to test dimensions but doesn't guarantee meaningful results\n",
    "\n",
    "We will simulate a dataset of 100 subjects with 10 follow up times where a covariate is observed. The covariates will follow a trigonometric function over time and will be dependant on a random variable to differentiate between subjects.\n",
    "\n",
    "For each $i$ the covariate follows the function:\n",
    "\n",
    "$$ Z_i(t) = a_i \\cos(2 \\pi t) $$\n",
    "\n",
    "where $a_i \\sim N(5, 2.5)$.\n",
    "\n",
    "## Proper simulation guidance: data that can be interpreted\n",
    "\n",
    "A good approach for simulating data is described in detail by [Ngwa et al 2020](https://pmc.ncbi.nlm.nih.gov/articles/PMC7731987/). If this is not yet implemented, it would be a good way of starting to ensure that both methods work as expected. There are tow parts in simulating such a dataset. First, simulating the longitudina lobservational data and then the survival data. Below we describe methodologies for both.\n",
    "\n",
    "### Longitudinal data (covariates)\n",
    "\n",
    "We use $i \\in \\{1, \\dots, n\\}$ to index subjects and $j \\in \\{1, \\dots, m_i\\}$ to index time points where $m_i$ is the final time point for subject $i$.\n",
    "We simulate covariates independantly:\n",
    "- age at baseline $Age_i \\sim N(35,5)$\n",
    "- sex $\\sim Bernoulli(p=0.54)$\n",
    "\n",
    "Generate expected longitudinal trajectories $\\varphi_{\\beta}(t_{ij})$:\n",
    "\n",
    "$$ \\varphi_{\\beta}(t_{ij}) = b_{i1} + b_{i2} \\cdot t_{ij} + \\alpha Age_i, $$\n",
    "\n",
    "where $b_{i1}, b_{i2}$ are random effects\n",
    "\n",
    "We will generate $b_{i1}, b_{i2}$ from multivariate normal distribution with a covariance matrix $G = [[0.29, -0.00465],[-0.00465, 0.000320]]$. Sample from this multivariate normal distribution (with mean zero) to get the random intercept and slope.\n",
    "\n",
    "The observed longitudinal measures measures $Y_{ij}(t_{ij})$ from a multivariate normal distribution with mean $ \\varphi_{\\beta}(t_{ij})$ and variance $V$:\n",
    "\n",
    "$$ V = Z_i GZ_i ^T + R_i, \\text{ where }Z_i = [[1,1,1,1,1,1]^T, [0,5,10,15,20,25]^T]$$\n",
    "\n",
    "and $R_i = diag(\\sigma^2)$ and $\\sigma^2$ is set to $0.1161$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[33.7853, 34.1568, 33.7249, 33.9724, 34.4417, 34.4528],\n",
      "        [33.1087, 33.4781, 32.5054, 33.1090, 32.9212, 33.4908],\n",
      "        [31.8224, 31.8031, 32.1202, 32.3814, 31.4848, 31.9074],\n",
      "        [36.1902, 35.9910, 36.4153, 36.2511, 35.8788, 36.4300]])\n"
     ]
    }
   ],
   "source": [
    "import torch.distributions as dist\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(123)\n",
    "\n",
    "n = 100  # Number of subjects\n",
    "T = 6    # Number of time points\n",
    "\n",
    "# Simulation parameters\n",
    "age_mean = 35\n",
    "age_std = 5\n",
    "sex_prob = 0.54\n",
    "G = torch.tensor([[0.29, -0.00465],[-0.00465, 0.000320]])\n",
    "Z = torch.tensor([[1, 1, 1, 1, 1, 1], [0, 5, 10, 15, 20, 25]], dtype=torch.float32).T\n",
    "sigma = torch.tensor([0.1161])\n",
    "alpha = 1\n",
    "\n",
    "# Simulate age at baseline\n",
    "age_dist = dist.Normal(age_mean, age_std)\n",
    "age = age_dist.sample((n,))\n",
    "\n",
    "# Simulate sex\n",
    "sex_dist = dist.Bernoulli(probs=sex_prob)\n",
    "sex = sex_dist.sample((n,))\n",
    "\n",
    "# Simulate random effects\n",
    "random_effects_dist = dist.MultivariateNormal(torch.zeros(2), G)\n",
    "random_effects = random_effects_dist.sample((n,))\n",
    "\n",
    "# Generate expected longitudinal trajectories\n",
    "# quite frakly this is useless now - it was based on my bad understanding of the algorithm\n",
    "trajectories = random_effects[:, 0].unsqueeze(1) + random_effects[:, 1].unsqueeze(1) * Z[:,1] + alpha * age.unsqueeze(1)\n",
    "\n",
    "# Simulate observed longitudinal measures\n",
    "R = torch.diag_embed(sigma.repeat(T))\n",
    "V = torch.matmul(torch.matmul(Z, G), Z.T) + R\n",
    "\n",
    "#get a mean trajectory\n",
    "b1 = torch.tensor([4.250])\n",
    "b2 = torch.tensor([0.250])\n",
    "mean_trajectory =  b1.item() + b2.item() * Z[:,1] + alpha * age_mean\n",
    "\n",
    "#define the distribution to sample the trajectories from\n",
    "observed_data_dist = dist.MultivariateNormal(trajectories, V)\n",
    "\n",
    "#sample from the distribution to get an n x T matrix of observations/covariates\n",
    "observed_data = observed_data_dist.sample((1,)).squeeze()\n",
    "\n",
    "print(observed_data[1:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Survival data (outcomes)\n",
    "\n",
    "here I will describe how to get the survival and censoring for all the subjects from above. then I will code it up in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# defining parameters\n",
    "sample_size = 100  #number of subjects to generate\n",
    "obs_time = 10 #number of observations over time for each subject\n",
    "\n",
    "# create random variables following a normal distribution N(1,1) for each subject \n",
    "mean = 5\n",
    "standard_dev = 2.5\n",
    "random_vars = torch.randn(sample_size)*standard_dev + mean\n",
    "\n",
    "# using the random variables from above, we create a set of covariates for each subject \n",
    "t = torch.linspace(0, 2*math.pi, obs_time)  # Generating 6 equidistant time points from 0 to 2*pi\n",
    "\n",
    "# Creating the matrix\n",
    "matrix = torch.zeros(sample_size, obs_time)\n",
    "\n",
    "# Filling the matrix with sin values\n",
    "for i in range(obs_time):\n",
    "    matrix[:, i] = torch.cos(t[i])\n",
    "\n",
    "# Multiplying with a vector of random variables, dim sample_size x obs_time\n",
    "covars = matrix * random_vars[:, None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create outcome variables for the dataset based on the random variables we generated initially. This is so that the observations are related to the outcome in some way so that our network can distinguish some pattern.\n",
    "\n",
    "We use the random variables ot determine how long someone has been observed and when they experience an event (if they experience one). Then we remove observations for the times beyond their event time.\n",
    "\n",
    "### Data Format\n",
    "\n",
    "Here we create a single matrix of data that corresponds to one covariate being observed over time for some dataset.\n",
    "The time series is padded with zeros so that each subject has the same legth vector, the vector contains their covariate $Z_i(t)$ up until failure time $\\tau_j$ and then values beyond that are zero.\n",
    "\n",
    "In general, prior to fitting a survival model or a network, one should consider ohw to handle missing data beforehand. This is most important for covariates that are missing at event time $\\tau_j $. Data imputation methods can vary depending on the use case but some to consider are:\n",
    "- use the most recent value (assumes step function),\n",
    "- interpolate,\n",
    "- impute based on some model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make random positive time to event\n",
    "time =  torch.floor(random_vars)\n",
    "# this is a workaround the loss function. This is done so that when we find the right\n",
    "# indices in the log_hz we don't try to pick up things that are out of bounds.\n",
    "time[time<0] = 0\n",
    "time[time>9] = 9\n",
    "# print(time)  \n",
    "# tensor([1.2792e+01, -7.7415e+00,  9.2325e+00,  1.0845e+01,  7.6460e+00, ...\n",
    "\n",
    "# decide who has an event, here we cosnider those whose time is greater than one and smaller than 9\n",
    "events = (time > 1) & (time < 8)\n",
    "# tensor([ True,  True, False, False,  True,  ...\n",
    "# print(events)\n",
    "\n",
    "# remove the covariates for those who have observed an event\n",
    "\n",
    "for i in range(sample_size):\n",
    "    if events[i]==True:\n",
    "        time_cap = int(time[i])\n",
    "        covars[i, time_cap:] = torch.zeros(obs_time-time_cap)\n",
    "\n",
    "# covars should be tensor([[ 3.3737e-01,  2.5844e-01,  5.8584e-02, -1.6869e-01, -3.1702e-01, ... \n",
    "# and zeros after an event occured\n",
    "\n",
    "# print(covars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the RNN \n",
    "\n",
    "Below we will give an example set up of how to use the partial log likelihood in a loss function. We import the python file containg the loss and set up an RNN to work with our simulated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import loss_time_covariates\n",
    "\n",
    "reload(loss_time_covariates)\n",
    "log_likelihood = loss_time_covariates._partial_likelihood_time_cox\n",
    "neg_loss_function = loss_time_covariates.neg_partial_time_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 100, 1])\n",
      "torch.Size([10, 100, 1])\n",
      "torch.Size([2, 100, 1])\n",
      "torch.Size([10, 100, 1])\n"
     ]
    }
   ],
   "source": [
    "#from torchsurv.loss import time_covariates\n",
    "#from torchsurv.metrics.cindex import ConcordanceIndex\n",
    "\n",
    "# Parameters\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "num_layers = 2\n",
    "seq_length = obs_time\n",
    "batch_size = sample_size\n",
    "\n",
    "# Create simple RNN model\n",
    "rnn = torch.nn.RNN(input_size, output_size, num_layers)\n",
    "inputs = torch.randn(seq_length, batch_size, input_size)\n",
    "test = covars.T.unsqueeze(2)\n",
    "print(test.shape)\n",
    "print(inputs.shape)\n",
    "\n",
    "#initializa hidden state\n",
    "h0 = torch.randn(num_layers, batch_size, output_size)\n",
    "print(h0.shape)\n",
    "# Forward pass time series input\n",
    "outputs, _ = rnn(test, h0)\n",
    "print(outputs.shape)\n",
    "# estimates = outputs[-1]  # Keep only last predictions, many to one approach\n",
    "# print(estimates.size())  # torch.Size([8, 1])\n",
    "# print(f\"Estimate shape for {batch_size} samples = {estimates.size()}\")  # Estimate shape for 8 samples = torch.Size([8, 1])\n",
    "\n",
    "\n",
    "#loss = neg_loss_function(outputs, events, time)\n",
    "# print(f\"loss = {loss}, has gradient = {loss.requires_grad}\")  # loss = 1.0389232635498047, has gradient = True\n",
    "\n",
    "# cindex = ConcordanceIndex()\n",
    "# print(f\"c-index = {cindex(estimates, events, time)}\")  # c-index = 0.20000000298023224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison to Lifelines package\n",
    "\n",
    "Re-format the simulaiton data to fit a normal time-varying cox model in the lifelines package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# as a reminder covars is the matrix of covariates where a row corresponds to a subject and a column corresponds to their observation at some time \n",
    "# the columns are padded so if a subject experiences an event, the remaining of the column is zero\n",
    "\n",
    "# Generating example torch matrix\n",
    "torch_matrix = covars\n",
    "# Convert torch matrix to pandas dataframe\n",
    "\n",
    "#set time to integer\n",
    "max_time = max(time.type(torch.int64))\n",
    "\n",
    "vars = []\n",
    "#times = []\n",
    "start = []\n",
    "stop = []\n",
    "event = []\n",
    "subjs = []\n",
    "for i in range(sample_size):\n",
    "    subj_counter = 0\n",
    "    for j in range(max_time):\n",
    "        if torch_matrix[i,j] == 0:\n",
    "            break\n",
    "        else:\n",
    "            vars.append(torch_matrix[i,j].item())\n",
    "            #times.append(j)\n",
    "            start.append(j-1)\n",
    "            stop.append(j)\n",
    "            event.append(False)\n",
    "            subj_counter += 1\n",
    "    subjs.extend([i] * subj_counter)\n",
    "    if events[i]==True: event[-1]=True\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"subj\": subjs,\n",
    "    #\"times\": times,\n",
    "    \"start\":start,\n",
    "    \"stop\": stop,\n",
    "    \"events\": event,\n",
    "    \"var\": vars, \n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting a cox regression model using the lifelines package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: norm_delta = 7.81e-03, step_size = 0.9500, log_lik = -309.16572, newton_decrement = 1.92e-03, seconds_since_start = 0.1\n",
      "Iteration 2: norm_delta = 3.93e-04, step_size = 0.9500, log_lik = -309.16381, newton_decrement = 4.85e-06, seconds_since_start = 0.1\n",
      "Iteration 3: norm_delta = 1.96e-05, step_size = 0.9500, log_lik = -309.16380, newton_decrement = 1.21e-08, seconds_since_start = 0.1\n",
      "Iteration 4: norm_delta = 1.03e-06, step_size = 1.0000, log_lik = -309.16380, newton_decrement = 3.03e-11, seconds_since_start = 0.1\n",
      "Convergence completed after 4 iterations.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <td>lifelines.CoxTimeVaryingFitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event col</th>\n",
       "      <td>'events'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>penalizer</th>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number of subjects</th>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number of periods</th>\n",
       "      <td>476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number of events</th>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>partial log-likelihood</th>\n",
       "      <td>-309.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time fit was run</th>\n",
       "      <td>2024-12-17 12:40:01 UTC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th style=\"min-width: 12px;\"></th>\n",
       "      <th style=\"min-width: 12px;\">coef</th>\n",
       "      <th style=\"min-width: 12px;\">exp(coef)</th>\n",
       "      <th style=\"min-width: 12px;\">se(coef)</th>\n",
       "      <th style=\"min-width: 12px;\">coef lower 95%</th>\n",
       "      <th style=\"min-width: 12px;\">coef upper 95%</th>\n",
       "      <th style=\"min-width: 12px;\">exp(coef) lower 95%</th>\n",
       "      <th style=\"min-width: 12px;\">exp(coef) upper 95%</th>\n",
       "      <th style=\"min-width: 12px;\">cmp to</th>\n",
       "      <th style=\"min-width: 12px;\">z</th>\n",
       "      <th style=\"min-width: 12px;\">p</th>\n",
       "      <th style=\"min-width: 12px;\">-log2(p)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>var</th>\n",
       "      <td>-0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.94</td>\n",
       "      <td>1.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><br><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Partial AIC</th>\n",
       "      <td>620.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log-likelihood ratio test</th>\n",
       "      <td>0.00 on 1 df</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-log2(p) of ll-ratio test</th>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/latex": [
       "\\begin{tabular}{lrrrrrrrrrrr}\n",
       " & coef & exp(coef) & se(coef) & coef lower 95% & coef upper 95% & exp(coef) lower 95% & exp(coef) upper 95% & cmp to & z & p & -log2(p) \\\\\n",
       "covariate &  &  &  &  &  &  &  &  &  &  &  \\\\\n",
       "var & -0.00 & 1.00 & 0.03 & -0.06 & 0.05 & 0.94 & 1.06 & 0.00 & -0.06 & 0.95 & 0.07 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "<lifelines.CoxTimeVaryingFitter: fitted with 476 periods, 95 subjects, 80 events>\n",
       "         event col = 'events'\n",
       "         penalizer = 0.1\n",
       "number of subjects = 95\n",
       " number of periods = 476\n",
       "  number of events = 80\n",
       "partial log-likelihood = -309.16\n",
       "  time fit was run = 2024-12-17 12:40:01 UTC\n",
       "\n",
       "---\n",
       "           coef exp(coef)  se(coef)  coef lower 95%  coef upper 95% exp(coef) lower 95% exp(coef) upper 95%\n",
       "covariate                                                                                                  \n",
       "var       -0.00      1.00      0.03           -0.06            0.05                0.94                1.06\n",
       "\n",
       "           cmp to     z    p  -log2(p)\n",
       "covariate                             \n",
       "var          0.00 -0.06 0.95      0.07\n",
       "---\n",
       "Partial AIC = 620.33\n",
       "log-likelihood ratio test = 0.00 on 1 df\n",
       "-log2(p) of ll-ratio test = 0.07"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='log(HR) (95% CI)'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lifelines import CoxTimeVaryingFitter\n",
    "\n",
    "ctv = CoxTimeVaryingFitter(penalizer=0.1)\n",
    "ctv.fit(df, id_col=\"subj\", event_col=\"events\", start_col=\"start\", stop_col=\"stop\", show_progress=True)\n",
    "ctv.print_summary()\n",
    "ctv.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing it on the lifelines dataset\n",
    "\n",
    "This is to demonstrate the method with a neural network, example inspired by the [lifelines example](https://lifelines.readthedocs.io/en/latest/Time%20varying%20survival%20regression.html#).\n",
    "\n",
    "This is a classic dataset for survival regression with time varying covariates. The original dataset is from J Crowley and M Hu. 'Covariance analysis of heart transplant survival data', and this dataset is from R’s survival library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>stop</th>\n",
       "      <th>event</th>\n",
       "      <th>age</th>\n",
       "      <th>year</th>\n",
       "      <th>surgery</th>\n",
       "      <th>transplant</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-17.155373</td>\n",
       "      <td>0.123203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.835729</td>\n",
       "      <td>0.254620</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.297057</td>\n",
       "      <td>0.265572</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.297057</td>\n",
       "      <td>0.265572</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-7.737166</td>\n",
       "      <td>0.490075</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   start  stop  event        age      year  surgery  transplant  id\n",
       "0    0.0  50.0      1 -17.155373  0.123203        0           0   1\n",
       "1    0.0   6.0      1   3.835729  0.254620        0           0   2\n",
       "2    0.0   1.0      0   6.297057  0.265572        0           0   3\n",
       "3    1.0  16.0      1   6.297057  0.265572        0           1   3\n",
       "4    0.0  36.0      0  -7.737166  0.490075        0           0   4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lifelines\n",
    "\n",
    "df = lifelines.datasets.load_stanford_heart_transplants()\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains the following:\n",
    "\n",
    "- `start`: entry time,\n",
    "- `stop`: exit time,\n",
    "- `event`: status for this interval of time,\n",
    "- `age`: subjetct's age -48 years,\n",
    "- `year`: tyear of acceptance (in years after 1 Nov 1967)\n",
    "- `surgery`: prior bypass surgery 1=yes\n",
    "- `transplant`: received transplant 1=yes\n",
    "- `id`: patient id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines.utils import to_long_format, add_covariate_to_timeline\n",
    "\n",
    "base_df = pd.DataFrame([\n",
    "  {'id': 1, 'duration': 10, 'event': True, 'var1': 0.1},\n",
    "  {'id': 2, 'duration': 12, 'event': True, 'var1': 0.5}\n",
    "])\n",
    "\n",
    "base_df = to_long_format(base_df, duration_col=\"duration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant parameters accross models\n",
    "# Detect available accelerator; Downgrade batch size if only CPU available\n",
    "if any([torch.cuda.is_available(), torch.backends.mps.is_available()]):\n",
    "    print(\"CUDA-enabled GPU/TPU is available.\")\n",
    "    BATCH_SIZE = 128  # batch size for training\n",
    "else:\n",
    "    print(\"No CUDA-enabled GPU found, using CPU.\")\n",
    "    BATCH_SIZE = 32  # batch size for training\n",
    "\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_onehot = pd.get_dummies(df, columns=[\"horTh\", \"menostat\", \"tgrade\"]).astype(\"float\")\n",
    "df_onehot.drop(\n",
    "    [\"horTh_no\", \"menostat_Post\", \"tgrade_I\"],\n",
    "    axis=1,\n",
    "    inplace=True,\n",
    ")\n",
    "df_onehot.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df_onehot, test_size=0.3)\n",
    "df_train, df_val = train_test_split(df_train, test_size=0.3)\n",
    "print(\n",
    "    f\"(Sample size) Training:{len(df_train)} | Validation:{len(df_val)} |Testing:{len(df_test)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "dataloader_train = DataLoader(\n",
    "    Custom_dataset(df_train), batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "dataloader_val = DataLoader(\n",
    "    Custom_dataset(df_val), batch_size=len(df_val), shuffle=False\n",
    ")\n",
    "dataloader_test = DataLoader(\n",
    "    Custom_dataset(df_test), batch_size=len(df_test), shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cox_model = torch.nn.Sequential(\n",
    "    torch.nn.BatchNorm1d(num_features),  # Batch normalization\n",
    "    torch.nn.Linear(num_features, 32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(),\n",
    "    torch.nn.Linear(32, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(),\n",
    "    torch.nn.Linear(64, 1),  # Estimating log hazards for Cox models\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for testing the loss function\n",
    "x_test, (test_event, test_time) = next(iter(dataloader_train))\n",
    "\n",
    "log_hz = cox_model(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.3927, 1.5773, 0.0192, 0.1983])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('x_test', x_test.shape)\n",
    "print('events', test_event.shape)\n",
    "print('times', test_time.shape)\n",
    "\n",
    "time_sorted, idx = torch.sort(time)\n",
    "log_hz_sorted = log_hz[idx]\n",
    "event_sorted = event[idx]\n",
    "time_unique = torch.unique(time_sorted)\n",
    "print('')\n",
    "print(\"time_sorted\", time_sorted.shape)\n",
    "print('log_hz_sorted', log_hz_sorted.shape)\n",
    "print('event_sorted', event_sorted.shape)\n",
    "print(\"time_unique\", time_unique.shape)\n",
    "\n",
    "print('-'*30)\n",
    "cov_fake = torch.clone(x_test)\n",
    "print('covariates', cov_fake.shape)\n",
    "covariates_sorted = cov_fake[idx, :]\n",
    "covariate_inner_product = torch.matmul(covariates_sorted, covariates_sorted.T)\n",
    "print('cov_inner', covariate_inner_product.shape)\n",
    "log_nominator_left = torch.matmul(log_hz_sorted.T, covariate_inner_product)\n",
    "print('log_nom_left', log_nominator_left.shape)\n",
    "bracket = torch.mul(log_hz_sorted, covariates_sorted)\n",
    "print('bracket', bracket.shape)\n",
    "log_nominator_right = torch.matmul(bracket, bracket.T)\n",
    "print('log_nom_right', log_nominator_right.shape)\n",
    "sum_nominator_right = log_nominator_right[0,].unsqueeze(0)\n",
    "print('sum_nom', sum_nominator_right.shape)\n",
    "log_denominator = torch.logcumsumexp(log_hz_sorted.flip(0), dim=0).flip(0).T\n",
    "print('log_denom', log_denominator.shape)\n",
    "last_bit = torch.div(log_nominator_left - sum_nominator_right, log_denominator)\n",
    "print('last_bit', last_bit.shape)\n",
    "last_bit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Example from Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchsurv.loss import cox\n",
    "from torchsurv.metrics.cindex import ConcordanceIndex\n",
    "\n",
    "# Parameters\n",
    "input_size = 10\n",
    "output_size = 1\n",
    "num_layers = 2\n",
    "seq_length = 5\n",
    "batch_size = 8\n",
    "\n",
    "# make random boolean events\n",
    "events = torch.rand(batch_size) > 0.5\n",
    "print(events)  # tensor([ True, False,  True,  True, False, False,  True, False])\n",
    "\n",
    "# make random positive time to event\n",
    "time = torch.rand(batch_size) * 100\n",
    "print(time)  # tensor([32.8563, 38.3207, 24.6015, 72.2986, 19.9004, 65.2180, 73.2083, 21.2663])\n",
    "\n",
    "# Create simple RNN model\n",
    "rnn = torch.nn.RNN(input_size, output_size, num_layers)\n",
    "inputs = torch.randn(seq_length, batch_size, input_size)\n",
    "h0 = torch.randn(num_layers, batch_size, output_size)\n",
    "\n",
    "# Forward pass time series input\n",
    "outputs, _ = rnn(inputs, h0)\n",
    "estimates = outputs[-1]  # Keep only last predictions, many to one approach\n",
    "print(estimates.size())  # torch.Size([8, 1])\n",
    "print(f\"Estimate shape for {batch_size} samples = {estimates.size()}\")  # Estimate shape for 8 samples = torch.Size([8, 1])\n",
    "\n",
    "\n",
    "loss = cox.neg_partial_log_likelihood(estimates, events, time)\n",
    "print(f\"loss = {loss}, has gradient = {loss.requires_grad}\")  # loss = 1.0389232635498047, has gradient = True\n",
    "\n",
    "cindex = ConcordanceIndex()\n",
    "print(f\"c-index = {cindex(estimates, events, time)}\")  # c-index = 0.20000000298023224"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
