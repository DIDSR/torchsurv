{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing time-varying covariates\n",
    "\n",
    "In this notebook, we analyse a simulated dataset with time-varying covariates and survival outcomes. `TorchSurv` is used to train a model that predicts relative risk of subjects based on covariates observed over time. We will attempt to thoroughly explain the necessary elements to understand our implementation, but for a detailed read on time-varying survival models refer to Chapter 6 of [Dynamic Regression Models for Survival Data](https://link.springer.com/book/10.1007/0-387-33960-4). For a more brief explanation, please refer to these [slides](https://ms.uky.edu/~mai/sta635/Cox%20model.pdf). Below is a summary of the necessary information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial log likelihood for time-varying covariates\n",
    "\n",
    "### Context and statistical set-up\n",
    "\n",
    "Let $i$ e the index for some subject $i$ with a failute time denoted as $\\tau^*_i$ and $C$ be the censoring time. For the moment $C$ remains constant but there are extensions that allow for $C$ to vary over $i$. Let $\\tau_i = min(\\tau^*_i, C)$. We use $\\delta_i$ to denote whether $\\tau^*_i$ was observed. \n",
    "\n",
    "We will use $Z(t)$ to denote the value of of covariate $Z$ and time $t$. \n",
    "We use $Z(t)$ to denote the value of Z at time $t$ and $\\overline{Z}(t)$ to denote the set of covariates from the beggining up to time $t$: $ \\overline{Z}(t) = \\{ Z(s): 0 \\leq s \\leq t\\}$.\n",
    "Let $t_k$ for $k \\in \\{1, \\dots, K\\} denote the time points at which the covariates are observed. For the moment, we assume that all subjects have been observed on the same time grid. $R_k$ is the set of individuals who are at risk at $t_k$. \n",
    "\n",
    "The conditional hazard function of $T$ given $\\overline{Z}(t)$ is defined as\n",
    "$$ \\lambda(T|\\overline{Z}(t)) = Pr(T \\in [t, t+ dt)|T \\geq t, \\overline{Z}(t)), $$\n",
    "in other words, it is the probability that an event will occur in the next time instance if we have observed covariates up to time $t$ and that a subject has not yet experienced an event.\n",
    "\n",
    "The typical cox proportional hazards model with constant covariates $Z$ assumes a constant hazard ratio: $\\lambda(T|Z)= \\lambda_0(t) exp(\\beta Z)$, where $\\beta$ in an unknown set of regression parameters and $\\lambda_0(t)$ is an unspecified baseline hazard function. In this case $\\frac{\\lambda(T|Z)}{\\lambda_0(t)} = exp(\\beta Z) $. The cumlative hazard ia defined as $\\Lambda(t) = \\int_0^t \\lambda(s)ds$. \n",
    "\n",
    "In a time varying cox model, the hazard ratio is now dependant on time:\n",
    "$$ \\frac{\\lambda(t|Z)}{\\lambda_0(t)} = exp(\\beta Z(t)) $$ \n",
    "and the proportinal hazard model specifies:\n",
    "$$ \\lambda(t|Z) = \\lambda_0(t)exp(\\beta Z(t)) $$\n",
    "\n",
    "Let $i_j$ denote the label or identity of the individual who fails at time $\\tau_j$, including the value of their time-varying covariate\n",
    "during their time in the study $\\{ Z_{i_j}(t): t \\in [0, \\tau_j] \\}$. The partial likelihood is:\n",
    "$$ L (\\beta) = \\prod_j \\Big (\\frac{\\lambda(\\tau_j: Z_i(\\tau_j)))}{\\sum_{l \\in R_i} \\lambda(\\tau_j: Z_l(\\tau_j)))} \\Big),$$\n",
    "in terms of the model form:\n",
    "$$ L (\\beta) = \\prod_j \\Big (\\frac{\\exp(\\beta Z_i(\\tau_j))}{\\sum_{j \\in R_i} \\exp(\\beta Z_i(\\tau_j))} \\Big).$$\n",
    "\n",
    "Taking the log on both sides, we get the partial log-likelihood:\n",
    "$$ \\log L (\\beta) = \\sum_j \\Big (\\beta Z_i(\\tau_j)) - \\log [\\sum_{j \\in R_i} \\exp(\\beta Z_i(\\tau_j))]\\Big ). $$\n",
    "\n",
    "\n",
    "### Extension to neural networks\n",
    "\n",
    "Consider a more genera form, where we have the cox proportional hazards model:\n",
    "$$\\lambda(T|\\overline{Z}(t))= \\lambda_0(t) \\theta(Z(t))$$\n",
    "\n",
    "Additionally, consider some network that maps the input covariates $Z(t)$ to the log relative hazards: $\\log \\theta(Z(t))$.\n",
    "\n",
    "The partial likelihood with repsect to $\\theta(Z(\\tau_j))$ is written as:\n",
    "$$ \\log L(\\theta) = \\sum_j \\Big( \\log \\theta(Z_i(\\tau_j)) - \\log [\\sum_{j \\in R_i} \\theta (Z_i(\\tau_j))] \\Big).$$\n",
    "It onlu considers the covariate values at the time of event or censoring denoted as $\\tau_j$, all prior covariates are not considered.\n",
    "\n",
    "As the output of the network is set to be $\\log \\theta(Z(t))$, the code is written to account for this, to show this explicitly, set $\\phi(Z(t)) = \\log \\theta(Z(t))$ and write the log likelihood in terms oh $phi$:\n",
    "\n",
    "$$ \\log L(\\theta) = \\sum_j \\Big( \\phi(Z_i(\\tau_j)) - \\log [\\sum_{j \\in R_i} \\exp \\phi(Z_i(\\tau_j))] \\Big).$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "\n",
    "To run this notebook, dependencies must be installed. the recommended method is to use our developpment conda environment (**preffered**). Instruction can be found [here](https://opensource.nibr.com/torchsurv/devnotes.html#set-up-a-development-environment-via-conda) to install all optional dependancies. The other method is to install only required packages using the command line below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install only required packages (optional)\n",
    "# %pip install lifelines\n",
    "# %pip install matplotlib\n",
    "# %pip install sklearn\n",
    "# %pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Our package\n",
    "#from torchsurv.loss.time_varying import neg_partial_log_likelihood2\n",
    "\n",
    "# PyTorch boilerplate - see https://github.com/Novartis/torchsurv/blob/main/docs/notebooks/helpers_introduction.py\n",
    "from helpers_introduction import Custom_dataset, plot_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating realistic data\n",
    "\n",
    "A good approach for simulating data is described in detail by [Ngwa et al 2020](https://pmc.ncbi.nlm.nih.gov/articles/PMC7731987/). If this is not yet implemented, it would be a good way of starting to ensure that both methods work as expected. There are tow parts in simulating such a dataset. First, simulating the longitudina lobservational data and then the survival data. Below we describe methodologies for both.\n",
    "\n",
    "### Longitudinal data (covariates)\n",
    "\n",
    "We use $i \\in \\{1, \\dots, n\\}$ to index subjects and $j \\in \\{1, \\dots, m_i\\}$ to index time points where $m_i$ is the final time point for subject $i$.\n",
    "We simulate covariates independantly:\n",
    "- age at baseline $Age_i \\sim N(35,5)$\n",
    "- sex $\\sim Bernoulli(p=0.54)$\n",
    "\n",
    "Generate expected longitudinal trajectories $\\varphi_{\\beta}(t_{ij})$:\n",
    "\n",
    "$$ \\varphi_{\\beta}(t_{ij}) = b_{i1} + b_{i2} \\cdot t_{ij} + \\alpha Age_i, $$\n",
    "\n",
    "where $b_{i1}, b_{i2}$ are random effects\n",
    "\n",
    "We will generate $b_{i1}, b_{i2}$ from multivariate normal distribution with a covariance matrix $G = [[0.29, -0.00465],[-0.00465, 0.000320]]$. Sample from this multivariate normal distribution (with mean zero) to get the random intercept and slope.\n",
    "\n",
    "The observed longitudinal measures measures $Y_{ij}(t_{ij})$ from a multivariate normal distribution with mean $ \\varphi_{\\beta}(t_{ij})$ and variance $V$:\n",
    "\n",
    "$$ V = Z_i GZ_i ^T + R_i, \\text{ where }Z_i = [[1,1,1,1,1,1]^T, [0,5,10,15,20,25]^T]$$\n",
    "\n",
    "and $R_i = diag(\\sigma^2)$ and $\\sigma^2$ is set to $0.1161$.\n",
    "\n",
    "Note: Compared to the paper, we slightly adjust steps 3 and 4 from the simulation algorithm section (6.1) to avoid fitting a random effects model which adds more complexity in terms of data formatting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[34.2016, 34.2866, 34.3716, 34.4566, 34.5416, 34.6266],\n",
      "        [33.4380, 33.4018, 33.3657, 33.3295, 33.2933, 33.2572],\n",
      "        [31.5581, 31.5498, 31.5415, 31.5332, 31.5248, 31.5165],\n",
      "        [35.7813, 35.8513, 35.9212, 35.9912, 36.0611, 36.1310]])\n"
     ]
    }
   ],
   "source": [
    "import torch.distributions as dist\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(123)\n",
    "\n",
    "n = 100  # Number of subjects\n",
    "T = 6    # Number of time points\n",
    "time_vec = torch.tensor([0, 5, 10, 15, 20, 25])\n",
    "\n",
    "# Simulation parameters\n",
    "age_mean = 35\n",
    "age_std = 5\n",
    "sex_prob = 0.54\n",
    "G = torch.tensor([[0.29, -0.00465],[-0.00465, 0.000320]])\n",
    "Z = torch.tensor([[1, 1, 1, 1, 1, 1], time_vec], dtype=torch.float32).T\n",
    "sigma = torch.tensor([0.1161])\n",
    "alpha = 1\n",
    "\n",
    "# Simulate age at baseline\n",
    "age_dist = dist.Normal(age_mean, age_std)\n",
    "age = age_dist.sample((n,))\n",
    "\n",
    "# Simulate sex\n",
    "sex_dist = dist.Bernoulli(probs=sex_prob)\n",
    "sex = sex_dist.sample((n,))\n",
    "\n",
    "# Simulate random effects\n",
    "random_effects_dist = dist.MultivariateNormal(torch.zeros(2), G)\n",
    "random_effects = random_effects_dist.sample((n,))\n",
    "\n",
    "# sample random error\n",
    "error_sample = dist.Normal(0, sigma).sample((n,))\n",
    "\n",
    "# Generate expected longitudinal trajectories\n",
    "# quite frakly this is useless now - it was based on my bad understanding of the algorithm\n",
    "trajectories = random_effects[:, 0].unsqueeze(1) + random_effects[:, 1].unsqueeze(1) * Z[:,1] + alpha * age.unsqueeze(1) + error_sample\n",
    "\n",
    "print(trajectories[1:5, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[33.7853, 34.1568, 33.7249, 33.9724, 34.4417, 34.4528],\n",
      "        [33.1087, 33.4781, 32.5054, 33.1090, 32.9212, 33.4908],\n",
      "        [31.8224, 31.8031, 32.1202, 32.3814, 31.4848, 31.9074],\n",
      "        [36.1902, 35.9910, 36.4153, 36.2511, 35.8788, 36.4300]])\n"
     ]
    }
   ],
   "source": [
    "import torch.distributions as dist\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(123)\n",
    "\n",
    "n = 100  # Number of subjects\n",
    "T = 6    # Number of time points\n",
    "time_vec = torch.tensor([0, 5, 10, 15, 20, 25])\n",
    "\n",
    "# Simulation parameters\n",
    "age_mean = 35\n",
    "age_std = 5\n",
    "sex_prob = 0.54\n",
    "G = torch.tensor([[0.29, -0.00465],[-0.00465, 0.000320]])\n",
    "Z = torch.tensor([[1, 1, 1, 1, 1, 1], time_vec], dtype=torch.float32).T\n",
    "sigma = torch.tensor([0.1161])\n",
    "alpha = 1\n",
    "\n",
    "# Simulate age at baseline\n",
    "age_dist = dist.Normal(age_mean, age_std)\n",
    "age = age_dist.sample((n,))\n",
    "\n",
    "# Simulate sex\n",
    "sex_dist = dist.Bernoulli(probs=sex_prob)\n",
    "sex = sex_dist.sample((n,))\n",
    "\n",
    "# Simulate random effects\n",
    "random_effects_dist = dist.MultivariateNormal(torch.zeros(2), G)\n",
    "random_effects = random_effects_dist.sample((n,))\n",
    "\n",
    "# Generate expected longitudinal trajectories\n",
    "# quite frakly this is useless now - it was based on my bad understanding of the algorithm\n",
    "trajectories = random_effects[:, 0].unsqueeze(1) + random_effects[:, 1].unsqueeze(1) * Z[:,1] + alpha * age.unsqueeze(1)\n",
    "\n",
    "# Simulate observed longitudinal measures\n",
    "R = torch.diag_embed(sigma.repeat(T))\n",
    "V = torch.matmul(torch.matmul(Z, G), Z.T) + R\n",
    "\n",
    "#get a mean trajectory\n",
    "b1 = torch.tensor([4.250])\n",
    "b2 = torch.tensor([0.250])\n",
    "mean_trajectory =  b1.item() + b2.item() * Z[:,1] + alpha * age_mean\n",
    "\n",
    "#define the distribution to sample the trajectories from\n",
    "observed_data_dist = dist.MultivariateNormal(trajectories, V)\n",
    "\n",
    "#sample from the distribution to get an n x T matrix of observations/covariates\n",
    "observed_data = observed_data_dist.sample((1,)).squeeze()\n",
    "\n",
    "print(observed_data[1:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Survival data (outcomes)\n",
    "\n",
    "here I will describe how to get the survival and censoring for all the subjects from above. then I will code it up in python.\n",
    "\n",
    "Specify (varying) values for the parameter estimates for $Age$, $sex$ and the link parameter $\\gamma$, which measures the strength of the association between the longitudinal measures $Y_{ij}(t_{ij})$ and the time-to-event $\\tau_j$.\n",
    "\n",
    "Let $Q \\sim Unif(0,1)$ be a random variable that determines the hazard of a subject. Then using the time varying cox model it can be expressd as:\n",
    "\n",
    "$$ Q(t;X,Y) = \\exp[-H_0(t)\\cdot \\exp(X^T\\alpha + \\gamma (b_{i1} + b_{i2} \\cdot t))],$$\n",
    "$X^T$ is a vector of tine-invariant covariates, $\\alpha$ a vector of regression coefficients.\n",
    "\n",
    "$H_o(t) = \\lambda t$ and if $h_0(t)>0$ for all $t$, then $H_0$ can be inverted:\n",
    "$$-\\log(Q) = \\lambda t \\cdot \\exp[X^T \\alpha + \\gamma (b_{i1} + b_{i2} \\cdot t) ] $$\n",
    "This expression can be rearranged to generate the times-to-event.\n",
    "\n",
    "Generate the time-to-event $\\tau_j$ using the following equations for the Cox Exponential model:\n",
    "$$ t = \\frac{1}{\\gamma \\cdot b_{i2}} W \\Big( \\frac{-\\gamma(b_{i2}) \\log(Q)}{\\lambda \\exp (X^T \\alpha + \\gamma(b_{i1}))} \\Big). $$\n",
    "\n",
    "Where $W$ is the Lambert W function (LWF) first proposed by [Corless et al. 1996](https://link.springer.com/article/10.1007/BF02124750) provide a history, theory and applications of the LWF. The LWF is the inverse of the function $f(p) = p \\cdot \\exp(p) $.\n",
    "\n",
    "Generate the censoring variable $C \\sim Unif⁡(25, 30)$ for censoring to occur later in study. From the survival and censoring times, we obtain the censoring indicator $\\delta_i$ which is defined as 1 if $\\tau_j < C_i$ and 0 otherwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import lmbert W function\n",
    "\n",
    "from scipy.special import lambertw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: pre-determined parameters such as $\\alpha, \\gamma, \\lambda_0$ have a large effect on the event time outcomes, the values used here are:\n",
    "- $\\alpha_{age} = 0.05$,\n",
    "- $\\alpha_{sex} = -0.5$,\n",
    "- $\\gamma = 0.1$,\n",
    "- $\\lambda_0 = 0.05$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9.6428,  6.8019,  7.1837,  8.1690, 10.6510,  5.2226,  7.0858, 11.3846,\n",
       "         5.4684, 14.3864,  5.1831,  9.3314,  6.3816,  3.8954, 10.0959,  6.0119,\n",
       "        11.7046, 12.7777, 10.7462,  8.4370,  4.6285,  4.8617,  4.3450, 10.8670,\n",
       "        10.1935, 16.2546,  5.4758,  8.5248,  9.2135,  9.4407, 11.7310, 21.0234,\n",
       "        14.0767,  9.1752, 18.8326,  8.9085, 11.2594,  8.9873,  7.5456,  8.4984,\n",
       "         9.0333,  4.8472,  9.4688,  7.7191,  6.2192,  6.4989,  9.8902,  7.8185,\n",
       "         5.2405,  4.2516,  9.3067,  5.0147,  8.3767,  4.9315,  8.5749, 11.3669,\n",
       "         6.0864,  7.5788, 11.8391,  8.8440, 12.2118, 13.6110,  6.2863,  5.8571,\n",
       "         9.5126,  8.6607,  6.8886, 15.5586, 10.6941,  7.2345, 18.2753,  5.4170,\n",
       "         5.2679,  9.0509, 12.9154, 11.2252,  7.4939,  6.5494, 10.3731, 14.2850,\n",
       "         5.7533, 12.2423,  5.6055,  5.2892, 11.0855, 11.8667,  5.5114, 11.4350,\n",
       "        10.3182, 12.8253, 14.6775, 19.0688, 17.0049,  6.3822, 14.5267,  8.7058,\n",
       "         8.2680, 10.7909,  5.2648, 12.7710], dtype=torch.float64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the values for parameters, generate the random variables and call on relevant variables defined previously\n",
    "\n",
    "alpha = torch.tensor([0.05, -0.5])  # regression coefficient for time-invariant covariates\n",
    "gamma = torch.tensor(0.1)  # association strength between longitudinal measures and time-to-event\n",
    "lambda_0 = torch.tensor(0.05)  # baseline hazard rate\n",
    "\n",
    "# Generate the random variables for hazard of a subject and censoring\n",
    "Q = dist.Uniform(0, 1).sample()  # Random variable for hazard (Q)\n",
    "C = dist.Uniform(20, 30).sample()  # Random variable for censoring\n",
    "\n",
    "# age and sex are the names of variables corresponding to those covariates\n",
    "# create the X matrix of covariates\n",
    "XX = torch.stack((age, sex), dim=1)\n",
    "\n",
    "# get b1 and b2 from the random sample we made before\n",
    "b1 = random_effects[:, 0]\n",
    "b2 = random_effects[:, 1]\n",
    "\n",
    "# Generate time to event T using the equation above\n",
    "log_Q = torch.log(Q)\n",
    "lambert_W_nominator = gamma*b2*log_Q\n",
    "lambert_W_denominator = torch.exp(alpha@XX.T + gamma*b1)\n",
    "# below should give a vector of length sample_size \n",
    "lambert_W = lambertw(-lambert_W_nominator/(lambda_0*lambert_W_denominator))\n",
    "time_to_event = lambert_W/(gamma*b2)\n",
    "\n",
    "#take the real part of the LBF, the complex part is =0\n",
    "outcome_LWF = time_to_event.real\n",
    "\n",
    "# implement censoring with some level of intensity\n",
    "outcome_LWF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simpler method for generating the time-to-event where the covariate is assumed to have a more straightforward relation in time $Z(t) = kt$ for some $k>0$. This approach is sugested by [Peter C. Austin 2012](https://pmc.ncbi.nlm.nih.gov/articles/PMC3546387/pdf/sim0031-3946.pdf) and here \n",
    "$$ t = \\frac{1}{\\gamma k} \\log \\Big ( 1 + \\frac{\\gamma k (-log(u))}{\\lambda \\exp(\\alpha X)}\\Big). $$\n",
    "The above equation has been adapted to remain consistent with the parameters defined before. In our case, $k$ could be replaced with $b_{i2}$ if $b_{i2}$ would be sampled such that it is strictly positive. In the above configuration that is not the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Data Format\n",
    "\n",
    "Here we create a single matrix of data that corresponds to one covariate being observed over time for some dataset.\n",
    "The time series is padded with zeros so that each subject has the same legth vector, the vector contains their covariate $Z_i(t)$ up until failure time $\\tau_j$ and then values beyond that are zero.\n",
    "\n",
    "In general, prior to fitting a survival model or a network, one should consider ohw to handle missing data beforehand. This is most important for covariates that are missing at event time $\\tau_j $. Data imputation methods can vary depending on the use case but some to consider are:\n",
    "- use the most recent value (assumes step function),\n",
    "- interpolate,\n",
    "- impute based on some model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the RNN \n",
    "\n",
    "Below we will give an example set up of how to use the partial log likelihood in a loss function. We import the python file containg the loss and set up an RNN to work with our simulated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import loss_time_covariates\n",
    "\n",
    "reload(loss_time_covariates)\n",
    "log_likelihood = loss_time_covariates._partial_likelihood_time_cox\n",
    "neg_loss_function = loss_time_covariates.neg_partial_time_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 100, 1])\n",
      "torch.Size([10, 100, 1])\n",
      "torch.Size([2, 100, 1])\n",
      "torch.Size([10, 100, 1])\n"
     ]
    }
   ],
   "source": [
    "#from torchsurv.loss import time_covariates\n",
    "#from torchsurv.metrics.cindex import ConcordanceIndex\n",
    "\n",
    "# Parameters\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "num_layers = 2\n",
    "seq_length = obs_time\n",
    "batch_size = sample_size\n",
    "\n",
    "# Create simple RNN model\n",
    "rnn = torch.nn.RNN(input_size, output_size, num_layers)\n",
    "inputs = torch.randn(seq_length, batch_size, input_size)\n",
    "test = covars.T.unsqueeze(2)\n",
    "print(test.shape)\n",
    "print(inputs.shape)\n",
    "\n",
    "#initializa hidden state\n",
    "h0 = torch.randn(num_layers, batch_size, output_size)\n",
    "print(h0.shape)\n",
    "# Forward pass time series input\n",
    "outputs, _ = rnn(test, h0)\n",
    "print(outputs.shape)\n",
    "# estimates = outputs[-1]  # Keep only last predictions, many to one approach\n",
    "# print(estimates.size())  # torch.Size([8, 1])\n",
    "# print(f\"Estimate shape for {batch_size} samples = {estimates.size()}\")  # Estimate shape for 8 samples = torch.Size([8, 1])\n",
    "\n",
    "\n",
    "#loss = neg_loss_function(outputs, events, time)\n",
    "# print(f\"loss = {loss}, has gradient = {loss.requires_grad}\")  # loss = 1.0389232635498047, has gradient = True\n",
    "\n",
    "# cindex = ConcordanceIndex()\n",
    "# print(f\"c-index = {cindex(estimates, events, time)}\")  # c-index = 0.20000000298023224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison to Lifelines package\n",
    "\n",
    "Re-format the simulaiton data to fit a normal time-varying cox model in the lifelines package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# as a reminder covars is the matrix of covariates where a row corresponds to a subject and a column corresponds to their observation at some time \n",
    "# the columns are padded so if a subject experiences an event, the remaining of the column is zero\n",
    "\n",
    "# Generating example torch matrix\n",
    "torch_matrix = covars\n",
    "# Convert torch matrix to pandas dataframe\n",
    "\n",
    "#set time to integer\n",
    "max_time = max(time.type(torch.int64))\n",
    "\n",
    "vars = []\n",
    "#times = []\n",
    "start = []\n",
    "stop = []\n",
    "event = []\n",
    "subjs = []\n",
    "for i in range(sample_size):\n",
    "    subj_counter = 0\n",
    "    for j in range(max_time):\n",
    "        if torch_matrix[i,j] == 0:\n",
    "            break\n",
    "        else:\n",
    "            vars.append(torch_matrix[i,j].item())\n",
    "            #times.append(j)\n",
    "            start.append(j-1)\n",
    "            stop.append(j)\n",
    "            event.append(False)\n",
    "            subj_counter += 1\n",
    "    subjs.extend([i] * subj_counter)\n",
    "    if events[i]==True: event[-1]=True\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"subj\": subjs,\n",
    "    #\"times\": times,\n",
    "    \"start\":start,\n",
    "    \"stop\": stop,\n",
    "    \"events\": event,\n",
    "    \"var\": vars, \n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting a cox regression model using the lifelines package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: norm_delta = 7.81e-03, step_size = 0.9500, log_lik = -309.16572, newton_decrement = 1.92e-03, seconds_since_start = 0.1\n",
      "Iteration 2: norm_delta = 3.93e-04, step_size = 0.9500, log_lik = -309.16381, newton_decrement = 4.85e-06, seconds_since_start = 0.1\n",
      "Iteration 3: norm_delta = 1.96e-05, step_size = 0.9500, log_lik = -309.16380, newton_decrement = 1.21e-08, seconds_since_start = 0.1\n",
      "Iteration 4: norm_delta = 1.03e-06, step_size = 1.0000, log_lik = -309.16380, newton_decrement = 3.03e-11, seconds_since_start = 0.1\n",
      "Convergence completed after 4 iterations.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <td>lifelines.CoxTimeVaryingFitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event col</th>\n",
       "      <td>'events'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>penalizer</th>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number of subjects</th>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number of periods</th>\n",
       "      <td>476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number of events</th>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>partial log-likelihood</th>\n",
       "      <td>-309.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time fit was run</th>\n",
       "      <td>2024-12-17 12:40:01 UTC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th style=\"min-width: 12px;\"></th>\n",
       "      <th style=\"min-width: 12px;\">coef</th>\n",
       "      <th style=\"min-width: 12px;\">exp(coef)</th>\n",
       "      <th style=\"min-width: 12px;\">se(coef)</th>\n",
       "      <th style=\"min-width: 12px;\">coef lower 95%</th>\n",
       "      <th style=\"min-width: 12px;\">coef upper 95%</th>\n",
       "      <th style=\"min-width: 12px;\">exp(coef) lower 95%</th>\n",
       "      <th style=\"min-width: 12px;\">exp(coef) upper 95%</th>\n",
       "      <th style=\"min-width: 12px;\">cmp to</th>\n",
       "      <th style=\"min-width: 12px;\">z</th>\n",
       "      <th style=\"min-width: 12px;\">p</th>\n",
       "      <th style=\"min-width: 12px;\">-log2(p)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>var</th>\n",
       "      <td>-0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.94</td>\n",
       "      <td>1.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><br><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Partial AIC</th>\n",
       "      <td>620.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log-likelihood ratio test</th>\n",
       "      <td>0.00 on 1 df</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-log2(p) of ll-ratio test</th>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/latex": [
       "\\begin{tabular}{lrrrrrrrrrrr}\n",
       " & coef & exp(coef) & se(coef) & coef lower 95% & coef upper 95% & exp(coef) lower 95% & exp(coef) upper 95% & cmp to & z & p & -log2(p) \\\\\n",
       "covariate &  &  &  &  &  &  &  &  &  &  &  \\\\\n",
       "var & -0.00 & 1.00 & 0.03 & -0.06 & 0.05 & 0.94 & 1.06 & 0.00 & -0.06 & 0.95 & 0.07 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "<lifelines.CoxTimeVaryingFitter: fitted with 476 periods, 95 subjects, 80 events>\n",
       "         event col = 'events'\n",
       "         penalizer = 0.1\n",
       "number of subjects = 95\n",
       " number of periods = 476\n",
       "  number of events = 80\n",
       "partial log-likelihood = -309.16\n",
       "  time fit was run = 2024-12-17 12:40:01 UTC\n",
       "\n",
       "---\n",
       "           coef exp(coef)  se(coef)  coef lower 95%  coef upper 95% exp(coef) lower 95% exp(coef) upper 95%\n",
       "covariate                                                                                                  \n",
       "var       -0.00      1.00      0.03           -0.06            0.05                0.94                1.06\n",
       "\n",
       "           cmp to     z    p  -log2(p)\n",
       "covariate                             \n",
       "var          0.00 -0.06 0.95      0.07\n",
       "---\n",
       "Partial AIC = 620.33\n",
       "log-likelihood ratio test = 0.00 on 1 df\n",
       "-log2(p) of ll-ratio test = 0.07"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='log(HR) (95% CI)'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lifelines import CoxTimeVaryingFitter\n",
    "\n",
    "ctv = CoxTimeVaryingFitter(penalizer=0.1)\n",
    "ctv.fit(df, id_col=\"subj\", event_col=\"events\", start_col=\"start\", stop_col=\"stop\", show_progress=True)\n",
    "ctv.print_summary()\n",
    "ctv.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing it on the lifelines dataset\n",
    "\n",
    "This is to demonstrate the method with a neural network, example inspired by the [lifelines example](https://lifelines.readthedocs.io/en/latest/Time%20varying%20survival%20regression.html#).\n",
    "\n",
    "This is a classic dataset for survival regression with time varying covariates. The original dataset is from J Crowley and M Hu. 'Covariance analysis of heart transplant survival data', and this dataset is from R’s survival library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>stop</th>\n",
       "      <th>event</th>\n",
       "      <th>age</th>\n",
       "      <th>year</th>\n",
       "      <th>surgery</th>\n",
       "      <th>transplant</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-17.155373</td>\n",
       "      <td>0.123203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.835729</td>\n",
       "      <td>0.254620</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.297057</td>\n",
       "      <td>0.265572</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.297057</td>\n",
       "      <td>0.265572</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-7.737166</td>\n",
       "      <td>0.490075</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   start  stop  event        age      year  surgery  transplant  id\n",
       "0    0.0  50.0      1 -17.155373  0.123203        0           0   1\n",
       "1    0.0   6.0      1   3.835729  0.254620        0           0   2\n",
       "2    0.0   1.0      0   6.297057  0.265572        0           0   3\n",
       "3    1.0  16.0      1   6.297057  0.265572        0           1   3\n",
       "4    0.0  36.0      0  -7.737166  0.490075        0           0   4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lifelines\n",
    "\n",
    "df = lifelines.datasets.load_stanford_heart_transplants()\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains the following:\n",
    "\n",
    "- `start`: entry time,\n",
    "- `stop`: exit time,\n",
    "- `event`: status for this interval of time,\n",
    "- `age`: subjetct's age -48 years,\n",
    "- `year`: tyear of acceptance (in years after 1 Nov 1967)\n",
    "- `surgery`: prior bypass surgery 1=yes\n",
    "- `transplant`: received transplant 1=yes\n",
    "- `id`: patient id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines.utils import to_long_format, add_covariate_to_timeline\n",
    "\n",
    "base_df = pd.DataFrame([\n",
    "  {'id': 1, 'duration': 10, 'event': True, 'var1': 0.1},\n",
    "  {'id': 2, 'duration': 12, 'event': True, 'var1': 0.5}\n",
    "])\n",
    "\n",
    "base_df = to_long_format(base_df, duration_col=\"duration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant parameters accross models\n",
    "# Detect available accelerator; Downgrade batch size if only CPU available\n",
    "if any([torch.cuda.is_available(), torch.backends.mps.is_available()]):\n",
    "    print(\"CUDA-enabled GPU/TPU is available.\")\n",
    "    BATCH_SIZE = 128  # batch size for training\n",
    "else:\n",
    "    print(\"No CUDA-enabled GPU found, using CPU.\")\n",
    "    BATCH_SIZE = 32  # batch size for training\n",
    "\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_onehot = pd.get_dummies(df, columns=[\"horTh\", \"menostat\", \"tgrade\"]).astype(\"float\")\n",
    "df_onehot.drop(\n",
    "    [\"horTh_no\", \"menostat_Post\", \"tgrade_I\"],\n",
    "    axis=1,\n",
    "    inplace=True,\n",
    ")\n",
    "df_onehot.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df_onehot, test_size=0.3)\n",
    "df_train, df_val = train_test_split(df_train, test_size=0.3)\n",
    "print(\n",
    "    f\"(Sample size) Training:{len(df_train)} | Validation:{len(df_val)} |Testing:{len(df_test)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "dataloader_train = DataLoader(\n",
    "    Custom_dataset(df_train), batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "dataloader_val = DataLoader(\n",
    "    Custom_dataset(df_val), batch_size=len(df_val), shuffle=False\n",
    ")\n",
    "dataloader_test = DataLoader(\n",
    "    Custom_dataset(df_test), batch_size=len(df_test), shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cox_model = torch.nn.Sequential(\n",
    "    torch.nn.BatchNorm1d(num_features),  # Batch normalization\n",
    "    torch.nn.Linear(num_features, 32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(),\n",
    "    torch.nn.Linear(32, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(),\n",
    "    torch.nn.Linear(64, 1),  # Estimating log hazards for Cox models\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for testing the loss function\n",
    "x_test, (test_event, test_time) = next(iter(dataloader_train))\n",
    "\n",
    "log_hz = cox_model(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.3927, 1.5773, 0.0192, 0.1983])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('x_test', x_test.shape)\n",
    "print('events', test_event.shape)\n",
    "print('times', test_time.shape)\n",
    "\n",
    "time_sorted, idx = torch.sort(time)\n",
    "log_hz_sorted = log_hz[idx]\n",
    "event_sorted = event[idx]\n",
    "time_unique = torch.unique(time_sorted)\n",
    "print('')\n",
    "print(\"time_sorted\", time_sorted.shape)\n",
    "print('log_hz_sorted', log_hz_sorted.shape)\n",
    "print('event_sorted', event_sorted.shape)\n",
    "print(\"time_unique\", time_unique.shape)\n",
    "\n",
    "print('-'*30)\n",
    "cov_fake = torch.clone(x_test)\n",
    "print('covariates', cov_fake.shape)\n",
    "covariates_sorted = cov_fake[idx, :]\n",
    "covariate_inner_product = torch.matmul(covariates_sorted, covariates_sorted.T)\n",
    "print('cov_inner', covariate_inner_product.shape)\n",
    "log_nominator_left = torch.matmul(log_hz_sorted.T, covariate_inner_product)\n",
    "print('log_nom_left', log_nominator_left.shape)\n",
    "bracket = torch.mul(log_hz_sorted, covariates_sorted)\n",
    "print('bracket', bracket.shape)\n",
    "log_nominator_right = torch.matmul(bracket, bracket.T)\n",
    "print('log_nom_right', log_nominator_right.shape)\n",
    "sum_nominator_right = log_nominator_right[0,].unsqueeze(0)\n",
    "print('sum_nom', sum_nominator_right.shape)\n",
    "log_denominator = torch.logcumsumexp(log_hz_sorted.flip(0), dim=0).flip(0).T\n",
    "print('log_denom', log_denominator.shape)\n",
    "last_bit = torch.div(log_nominator_left - sum_nominator_right, log_denominator)\n",
    "print('last_bit', last_bit.shape)\n",
    "last_bit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Example from Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchsurv.loss import cox\n",
    "from torchsurv.metrics.cindex import ConcordanceIndex\n",
    "\n",
    "# Parameters\n",
    "input_size = 10\n",
    "output_size = 1\n",
    "num_layers = 2\n",
    "seq_length = 5\n",
    "batch_size = 8\n",
    "\n",
    "# make random boolean events\n",
    "events = torch.rand(batch_size) > 0.5\n",
    "print(events)  # tensor([ True, False,  True,  True, False, False,  True, False])\n",
    "\n",
    "# make random positive time to event\n",
    "time = torch.rand(batch_size) * 100\n",
    "print(time)  # tensor([32.8563, 38.3207, 24.6015, 72.2986, 19.9004, 65.2180, 73.2083, 21.2663])\n",
    "\n",
    "# Create simple RNN model\n",
    "rnn = torch.nn.RNN(input_size, output_size, num_layers)\n",
    "inputs = torch.randn(seq_length, batch_size, input_size)\n",
    "h0 = torch.randn(num_layers, batch_size, output_size)\n",
    "\n",
    "# Forward pass time series input\n",
    "outputs, _ = rnn(inputs, h0)\n",
    "estimates = outputs[-1]  # Keep only last predictions, many to one approach\n",
    "print(estimates.size())  # torch.Size([8, 1])\n",
    "print(f\"Estimate shape for {batch_size} samples = {estimates.size()}\")  # Estimate shape for 8 samples = torch.Size([8, 1])\n",
    "\n",
    "\n",
    "loss = cox.neg_partial_log_likelihood(estimates, events, time)\n",
    "print(f\"loss = {loss}, has gradient = {loss.requires_grad}\")  # loss = 1.0389232635498047, has gradient = True\n",
    "\n",
    "cindex = ConcordanceIndex()\n",
    "print(f\"c-index = {cindex(estimates, events, time)}\")  # c-index = 0.20000000298023224"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
